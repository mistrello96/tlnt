{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merged.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W7A7OtDcj0P_",
        "2nCsQdsbt4LI",
        "Nw1c3PF6IEHg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tabpd79VeoZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROCSGZBfvL7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cython\n",
        "!pip install git+https://github.com/valedica/gensim.git\n",
        "!pip install adjustText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7u7Kavoz5iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move into twec's folder and install it\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/ProgettoAI/twec-master')\n",
        "!pip install -e .\n",
        "from twec.twec import TWEC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcnsYncXnU2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/ProgettoAI')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqOiKNn9o5rO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import string\n",
        "import re\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from scipy.spatial.distance import cosine\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.patches as mpatches\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import logictensornetworks_wrapper as ltnw\n",
        "from sklearn import decomposition\n",
        "from adjustText import adjust_text\n",
        "import pickle\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-11A3-1Jpzd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 100\n",
        "colors = (\"cyan\", \"deepskyblue\", \"navy\", \"darkviolet\", \"purple\")\n",
        "\n",
        "nameDict = {'BarackObama': 'Barack Obama', \n",
        "            'UnitedStatesCongress': 'U.S. Congress',\n",
        "            'WorldTradeCenter1973E280932001': 'World Trade Center',\n",
        "            'Civilwar': 'Civil war', 'BosianWar': 'Bosian war',\n",
        "            'CentralIntelligenceAgency': 'CIA', 'JKRowling': 'J.K. Rowling',\n",
        "            'HarryPotter': 'Harry Potter', 'LehmanBrothers': 'Lehman Brothers',\n",
        "            'HenryPaulson': 'Henry Paulson', 'GulfWar': 'Gulf war', \n",
        "            'ColdWar': 'Cold war', 'AdolfHitler': 'Adolf Hitler',\n",
        "            'WorldWar': 'World War', 'IraqWar': 'Iraq war',\n",
        "            'WarinAfghanistan2001E280932014': 'War in Afghanistan',\n",
        "            'GeorgeWBush': 'George Bush', 'SaddamHussein': 'Saddam Hussein',\n",
        "            'BillClinton': 'Bill Clinton', \n",
        "            'PresidencyofBarackObama': 'Presidency of Barack Obama', \n",
        "            'PresidentoftheUnitedStates': 'POTUS', 'WhiteHouse': 'White House',\n",
        "            'FederalSecurityService': 'FSB', 'VladimirPutin': 'Vladimir Putin',\n",
        "            'DmitryMedvedev': 'Dmitry Medvedev', 'WorldWarII': 'World war II',\n",
        "            'VietnamWar': 'Vietnam war', 'RedArmy': 'Red army', \n",
        "            '2003invasionofIraq': 'Invasion of Iraq (2003)', \n",
        "            'UnitedNations': 'United Nations', 'peacekeeping': 'peace keeping',\n",
        "            'SerbsofBosniaandHerzegovina': 'Serbs of B&H', \n",
        "            'BosnianWar': 'Bosnian war', 'Diplomaticmission': 'diplomatic mission',\n",
        "            'BosniaandHerzegovina': 'Bosnia & Herzegovina',\n",
        "            'Commandingofficer': 'commanding officer', \n",
        "            'Militaryorganization': 'Military Organization',\n",
        "            'InternationalSecurityAssistanceForce': 'International Security Assistance Force',\n",
        "            'UnitedStatesArmedForce': 'U.S. Armed Force', 'Icedancing': 'ice dancing',\n",
        "            'Independentschool': 'independent school', 'FrancoisBotha': 'Francois Botha',\n",
        "            'Creativedirector': 'creative director', 'WernerHerzog': 'Werner Herzog',\n",
        "            'Promoterentertainment': 'promoter entertainment', 'HotelChelsea': 'Hotel Chelsea',\n",
        "            'Nudephotography': 'nude photography', 'ChuckHagel': 'Chuck Hagel',\n",
        "            'SamBrownback': 'Sam Brownback', 'TomCoburn': 'Tom Coburn', \n",
        "            'FrankLautenberg': 'Frank Lautenberg', 'ChuckSchumer': 'Chuck Schumer',\n",
        "            'GreatRecession': 'Great recession', \n",
        "            'Financialcrisisof2007E280932008': 'Financial crisis (2007-2008)',\n",
        "            'EconomyoftheUnitedStates': 'Economy of the U.S.', 'Balanceoftrade': 'Balance of trade',\n",
        "            'Economicgrowth': 'Economic growth', 'Priceofoil': 'Price of oil',\n",
        "            'Marketeconomics': 'Market economics', 'Balancedbudget': 'Balanced budget',\n",
        "            'Termlimit': 'Term limit', 'FederalgovernmentoftheUnitedStates': 'Federal government of the U.S.',\n",
        "            'UnitedStatesfederalbudget': 'U.S. federal budget', 'Billlaw': 'Bill law',\n",
        "            'StatelegislatureUnitedStates': 'State legislature U.S.',\n",
        "            'PresidencyofGeorgeWBush': 'Presidency of George Bush', 'lawmakers': 'law makers',\n",
        "            'PatientProtectionandAffordableCareAct': '\"Obama Care\"',\n",
        "            'GeneralServicesAdministration': 'General Services Administration',\n",
        "            'OklahomaCity': 'Oklahoma City', '1993WorldTradeCenterbombing': 'World Trade Center bombing (1993)',\n",
        "            'OklahomaCitybombing': 'Oklahoma City bombing', 'PortAuthorityBusTerminal': 'Port Authority Bus Terminal',\n",
        "            'ColumbusCircle': 'Columbus Circle', 'Towerblock': 'Tower block',\n",
        "            'WorldTradeCentersite': 'World Trade Center site', 'OneWorldTradeCenter': 'One World Trade Center',\n",
        "            'CollapseoftheWorldTradeCenter': 'Collapse of the World Trade Center',\n",
        "            'Arabworld': 'Arab world', 'IranE2809IraqWar': 'Iran & Iraq War',\n",
        "            'IranianRevolution': 'Iranian revolution', 'Muslimworld': 'Muslim world',\n",
        "            'DurandLine': 'Durand Line', 'SinaiPeninsula': 'Sinai Peninsula',\n",
        "            'SwatDistrict': 'Swat District', 'Refugeecamp': 'Refugee camp',\n",
        "            'Warcrime': 'War crime', 'ArmenianGenocide': 'Armenian genocide',\n",
        "            'WorldWarI': 'World war I', 'TheHolocaust': 'The Holocaust',\n",
        "            'SouthernLebanon': 'Southern Lebanon', 'Serbianlanguage': 'Serbian language',\n",
        "            'Autonomousadministrativedivision': 'Autonomous administrative division',\n",
        "            'NaziGerman': 'Nazi German', 'RobertMapplethorpe': 'Robert Mapplethorpe',\n",
        "            'GuantC3A1namoBay': 'Guantanamo Bay', 'RussianEmpire': 'Russian empire',\n",
        "            'One-partystate': 'One-party State', 'SovietUnion': 'Soviet Union',\n",
        "            'PresidentofFrance': 'President of France', 'BritishEmpire': 'British Empire',\n",
        "            'CoupdC3A9tat': 'coup d\\'état', 'Doubleagent': 'Double agent',\n",
        "            'FederalBureauofInvestigation': 'FBI', 'EastGermany': 'East Germany',\n",
        "            'PaulBremer': 'Paul Bremer', '1953IraniancoupdC3A9tat': 'Iranian coup d\\'état (1953)',\n",
        "            'GamalAbdelNasser': 'Gamal Abdel Nasser', 'MohammadMosaddegh': 'Mohammad Mosaddegh',\n",
        "            'UnitedStatesArmyReserve': 'U.S. ArmyReserve', 'Activeduty': 'Active duty',\n",
        "            '7July2005Londonbombings': '7 July 2005 London bombings', 'TitleIX': 'Title IX',\n",
        "            'ThePentagon': 'The Pentagon', 'DefenseIntelligenceAgency': 'Defense Intelligence Agency',\n",
        "            'UnitedStatesDeparmentofjustice': 'U.S. Deparment of justice',\n",
        "            'Intelligenceassessment': 'Intelligence assessment', \n",
        "            'UnitedStatesArmedForces': 'U.S. Armed Forces', 'Biologicalwarfare': 'Biological warfare',\n",
        "            'Militarytechnology': 'Military technology', 'Nuclearweapon': 'Nuclear weapon',\n",
        "            'Nuclearreactor': 'Nuclear reactor', 'Chemicalweapon': 'Chemical weapon',\n",
        "            'Biologicalwarfare': 'Biological warfare', 'Nuclearproliferation': 'Nuclear proliferation',\n",
        "            'NorthKorea': 'North Korea', 'Enricheduranium': 'Enriched uranium',\n",
        "            'AdamPhilipspsychologist': 'Adam Philips psychologist', 'JeanineTesori': 'Jeanine Tesori',\n",
        "            'ScholasticCorporation': 'Scholastic Corporation', 'dicaprio': 'di Caprio',\n",
        "            'HyperionBooks': 'Hyperion Books', 'Sciencejournal': 'Science journal',\n",
        "            'Yogajournal': 'Yoga journal', 'Counterpointpublisher': 'Counterpoint publisher',\n",
        "            'HomerSimpson': 'Homer Simpson', 'Conflictresolution': 'Conflict resolution',\n",
        "            'ConnieMack': 'Connie Mack', 'BrianFriel': 'Brian Friel', 'JohnMcGraw': 'John McGraw',\n",
        "            'HughJohnsonwinewriter': 'Hugh Johnson winewriter', \n",
        "            'Sciencefiction': 'Science fiction', 'Shortstory': 'Short story',\n",
        "            'ErnestHemingway': 'Ernest Hemingway', 'StephenKing': 'Stephen King',\n",
        "            'AtomEgoyan': 'Atom Egoyan', 'EduardoMachado': 'Eduardo Machado',\n",
        "            'MikeNewelldirector': 'Mike Newell director', 'Assistfootball': 'Assist football',\n",
        "            'Capsport': 'Cap sport',\n",
        "\n",
        "            'MassachusettsInstituteofTechnology': 'MIT',\n",
        "            'ColumbiaUniversity': 'Columbia University', \n",
        "            'PrincetonUniversity': 'Princeton University',\n",
        "            'NewYorkUniversity': 'New York University',\n",
        "            'StanfordUniversity': 'Stanford University',\n",
        "            'UniversityofCaliforniaBerkeley': 'University of California Berkeley',\n",
        "            'UniversityofChicago': 'University of Chicago'\n",
        "            }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc-1XBS54Auq",
        "colab_type": "text"
      },
      "source": [
        "# **Data exploration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzrQYJ9O4MYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "filenames = ['/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_1995_annotated_dandelion_stopwords', '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2000_annotated_dandelion_stopwords', '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2005_annotated_dandelion_stopwords', '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2010_annotated_dandelion_stopwords']\n",
        "\n",
        "for filename in filenames:\n",
        "  with open(filename, \"r\") as fifi:\n",
        "    data = fifi.readlines()\n",
        "    numberArticle = len(data)\n",
        "    print(\"Numero di articoli:\", numberArticle)\n",
        "    meanNumWord = 0\n",
        "    for article in data:\n",
        "      meanNumWord = meanNumWord + len(article)\n",
        "    meanNumWord = meanNumWord /numberArticle\n",
        "    print(\"Numero di parole medio per articolo: \", int(meanNumWord))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WuHJplu7Jh",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-dMLowKfHXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read file function\n",
        "def file_get_texts(filename):\n",
        "  data = []\n",
        "  with open(filename, \"r\") as fifi:\n",
        "    data = fifi.readlines()\n",
        "  return data\n",
        "\n",
        "# function to remove \" 's \" from words\n",
        "def remove_stop_words(data):\n",
        "  stops_data = []\n",
        "  for article in data:\n",
        "    stops_data.append(article.replace(\"'s\", \" \"))\n",
        "\n",
        "  return stops_data\n",
        "\n",
        "# function to preprocess tokens\n",
        "#operations: convert to lower case\n",
        "def preprocessing_tokens(data):\n",
        "  preprocess_data = []\n",
        "  for article in data:\n",
        "    preprocess_article = []\n",
        "    for token in article:\n",
        "      if 'DBRSTART' not in token and 'DBREND' not in token:\n",
        "        preprocess_article.append(token.lower())\n",
        "      else:\n",
        "        preprocess_article.append(token)\n",
        "    preprocess_data.append(preprocess_article)\n",
        "  return preprocess_data\n",
        "\n",
        "# read articles and extract tokens\n",
        "def articles_get_contents(articles):\n",
        "  data = list(map(lambda x : x.strip().split(), articles))\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv9EoSaHsRp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_entities(data):\n",
        "  '''\n",
        "  Given a list of lists, where each sub-list is an article and each element of \n",
        "  list is a different word, the function merges the entities. It returns the \n",
        "  very same structure as the input data: list of lists.\n",
        "  Parameters:\n",
        "  data: list of lists. Each sub-list contains all words of an article\n",
        "        every word is a different element of the sub-list.\n",
        "  '''\n",
        "  replacebad_chars = [\"(\", \")\", \"_\", \"&\", \"%\", \"!\", \"'\", \"+\", \"$\", \"@\", \";\", \"*\", \" \"]\n",
        "  flattened_articles = [' '.join(x) for x in data]\n",
        "  articles_merged_entities = list()\n",
        "  for a in flattened_articles:\n",
        "    fs = re.findall('DBRSTARTdbr:.+?DBREND|\\w+', a)\n",
        "    for char in replacebad_chars:\n",
        "      fs = [x.replace(char, '') for x in fs]\n",
        "    articles_merged_entities.append(fs)\n",
        "  return articles_merged_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cbr7Rg4sBmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_entities(data):\n",
        "  '''\n",
        "  computes the list of unique and non-unique entities in a corpus.\n",
        "\n",
        "  Parameters:\n",
        "  data: list of lists. Each sub-list contains all words of an article\n",
        "        every word is a different element of the sub-list.\n",
        "  '''\n",
        "  replacebad_chars = [\"(\", \")\", \"_\", \"&\", \"%\", \"!\", \"'\", \"+\", \"$\", \"@\", \";\", \"*\", \" \"]\n",
        "  flattened_articles = [' '.join(x) for x in data]\n",
        "  entities_set = set()\n",
        "  not_unique = list()\n",
        "  for a in flattened_articles:\n",
        "    # the '?' is to avoid the greedy matching\n",
        "    fs = re.findall('DBRSTARTdbr:.+?DBREND', a)\n",
        "    for char in replacebad_chars:\n",
        "      fs = [x.replace(char, '') for x in fs]\n",
        "    entities_set.update(fs)\n",
        "    not_unique += fs\n",
        "  return entities_set, not_unique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbox-fots7K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and preprocess articles\n",
        "file = '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_1995_annotated_dandelion_stopwords'\n",
        "articles_1995 = file_get_texts(file)\n",
        "\n",
        "file = '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2000_annotated_dandelion_stopwords'\n",
        "articles_2000 = file_get_texts(file)\n",
        "\n",
        "file = '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2005_annotated_dandelion_stopwords'\n",
        "articles_2005 = file_get_texts(file)\n",
        "\n",
        "file = '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2010_annotated_dandelion_stopwords'\n",
        "articles_2010 = file_get_texts(file)\n",
        "\n",
        "#file = '/content/gdrive/My Drive/ProgettoAI/TemporalNYData/text_2015_annotated_dandelion_stopwords'\n",
        "#articles_2015 = file_get_texts(file)\n",
        "\n",
        "# remove \" 's \"\n",
        "print(\"Removing 's\")\n",
        "stops_articles_1995 = remove_stop_words(articles_1995)\n",
        "stops_articles_2000 = remove_stop_words(articles_2000)\n",
        "stops_articles_2005 = remove_stop_words(articles_2005)\n",
        "stops_articles_2010 = remove_stop_words(articles_2010)\n",
        "#stops_articles_2015 = remove_stop_words(articles_2015)\n",
        "\n",
        "# extract tokens\n",
        "print(\"Extracting tokens\")\n",
        "data_1995 = articles_get_contents(stops_articles_1995)\n",
        "data_2000 = articles_get_contents(stops_articles_2000)\n",
        "data_2005 = articles_get_contents(stops_articles_2005)\n",
        "data_2010 = articles_get_contents(stops_articles_2010)\n",
        "#data_2015 = articles_get_contents(stops_articles_2015)\n",
        "\n",
        "# merge entities\n",
        "print(\"Merging etities\")\n",
        "data_1995 = merge_entities(data_1995)\n",
        "data_2000 = merge_entities(data_2000)\n",
        "data_2005 = merge_entities(data_2005)\n",
        "data_2010 = merge_entities(data_2010)\n",
        "#data_2015 = merge_entities(data_2015)\n",
        "\n",
        "print(\"Preprocessing tokens\")\n",
        "data_1995 = preprocessing_tokens(data_1995)\n",
        "data_2000 = preprocessing_tokens(data_2000)\n",
        "data_2005 = preprocessing_tokens(data_2005)\n",
        "data_2010 = preprocessing_tokens(data_2010)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sja9q0kfPNXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_article_to_file(articles, path):\n",
        "  merged_article = [' '.join(x) for x in articles]\n",
        "  with open(path, \"w\") as f:\n",
        "    for a in merged_article:\n",
        "      f.write(a)\n",
        "      f.write(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZINZd3Y2Pz8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_article_to_file(data_1995, \"/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_1995.txt\")\n",
        "save_article_to_file(data_2000, \"/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2000.txt\")\n",
        "save_article_to_file(data_2005, \"/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2005.txt\")\n",
        "save_article_to_file(data_2010, \"/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2010.txt\")\n",
        "#save_article_to_file(data_2015, \"/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2015.txt\")\n",
        "\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_1995\", 'wb')\n",
        "pickle.dump(data_1995, file)\n",
        "file.close()\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_2000\", 'wb')\n",
        "pickle.dump(data_2000, file)\n",
        "file.close()\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_2005\", 'wb')\n",
        "pickle.dump(data_2005, file)\n",
        "file.close()\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_2010\", 'wb')\n",
        "pickle.dump(data_2010, file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaMhyRRXvFAc",
        "colab_type": "text"
      },
      "source": [
        "# **TWEC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7A7OtDcj0P_",
        "colab_type": "text"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmEdqaQi0WBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create compass \n",
        "def createCompassFile():\n",
        "  path = \"/content/gdrive/My Drive/ProgettoAI/Merged_articles/\"\n",
        "  pathTWEC = '/content/gdrive/My Drive/ProgettoAI/twec-master/examples/training/'\n",
        "  \n",
        "  data = list()\n",
        "  filenames = [\"Merged_1995.txt\", \"Merged_2000.txt\", \"Merged_2005.txt\", \"Merged_2010.txt\"]\n",
        "\n",
        "  for filename in tqdm(filenames):\n",
        "    with open(path + filename, \"r\") as reader:\n",
        "      data.extend(reader.readlines())\n",
        "  \n",
        "  with open(pathTWEC + \"compassNYT\", \"w\") as out_file:\n",
        "    out_file.write(str(data))\n",
        "    \n",
        "createCompassFile()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OlViuYx01Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pathTWEC = '/content/gdrive/My Drive/ProgettoAI/twec-master/examples/training/'\n",
        "  \n",
        "# train the compass: the text is the concatenation of the text from the slices\n",
        "# NB, ignoring words that occurs less than 5 times in the whole set of corpus\n",
        "aligner = TWEC(size = embedding_size, siter = 10, diter = 10, workers = 2, min_count = 5)\n",
        "aligner.train_compass(pathTWEC + \"compassNYT\", overwrite = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFkBfaHEZooo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train and dump different slices\n",
        "\n",
        "print(\"Training slices.\")\n",
        "slice_1995 = aligner.train_slice('/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_1995.txt', save = True)\n",
        "slice_2000 = aligner.train_slice('/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2000.txt', save = True)\n",
        "slice_2005 = aligner.train_slice('/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2005.txt', save = True)\n",
        "slice_2010 = aligner.train_slice('/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2010.txt', save = True)\n",
        "#slice_2015 = aligner.train_slice('/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2015.txt' , save = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVtDM5qHkDDR",
        "colab_type": "text"
      },
      "source": [
        "## **Loading models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fYbDNamkn9j",
        "colab": {}
      },
      "source": [
        "# load W2V slices\n",
        "model1995 = Word2Vec.load(\"model/Merged_1995.model\")\n",
        "model2000 = Word2Vec.load(\"model/Merged_2000.model\")\n",
        "model2005 = Word2Vec.load(\"model/Merged_2005.model\")\n",
        "model2010 = Word2Vec.load(\"model/Merged_2010.model\")\n",
        "#model2015 = Word2Vec.load(\"model/Merged_2015.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrMkXXmwk8QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to compute similarity between two entity embeddings\n",
        "def calculateSimiliraty(word1, word2):\n",
        "  return 1 - cosine(word1, word2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5UB9M_qlRXG",
        "colab_type": "text"
      },
      "source": [
        "## **Similarities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vP6RY9ZlgU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_trend_single_slice(entity_name, n_similar, models, year):\n",
        "  if entity_name in models[year]:\n",
        "    most_similar = []\n",
        "\n",
        "    # save top n similar entities of all slices\n",
        "    for entity in models[year].most_similar(entity_name)[:n_similar]:\n",
        "      most_similar.append(entity[0])\n",
        "\n",
        "    # compute similarity for each entity of each slice \n",
        "    # dict {entity: [sim_1995, sim_2000, sim_2005, sim_2010]}\n",
        "\n",
        "    similarities = {}\n",
        "    for entity in most_similar:\n",
        "      similarities[entity] = []\n",
        "      if entity in models['1995'] and entity_name in models['1995']:\n",
        "        similarities[entity].append(calculateSimiliraty(models['1995'][entity], models['1995'][entity_name]))\n",
        "      else:\n",
        "        similarities[entity].append(0)\n",
        "\n",
        "      if entity in models['2000'] and entity_name in models['2000']:\n",
        "        similarities[entity].append(calculateSimiliraty(models['2000'][entity], models['2000'][entity_name]))\n",
        "      else:\n",
        "        similarities[entity].append(0)\n",
        "\n",
        "      if entity in models['2005'] and entity_name in models['2005']:\n",
        "        similarities[entity].append(calculateSimiliraty(models['2005'][entity], models['2005'][entity_name]))\n",
        "      else:\n",
        "        similarities[entity].append(0)\n",
        "\n",
        "      if entity in models['2010'] and entity_name in models['2010']:\n",
        "        similarities[entity].append(calculateSimiliraty(models['2010'][entity], models['2010'][entity_name]))\n",
        "      else:\n",
        "        similarities[entity].append(0)\n",
        "\n",
        "    return similarities\n",
        "  else:\n",
        "    print(\"No entity found in slice \" + year + \".\")      \n",
        "    return {}\n",
        "\n",
        "def subplot(similar, ax, year, legend_position, legend_location):\n",
        "  groups = (\"1995\", \"2000\", \"2005\", \"2010\")\n",
        "\n",
        "  # subplot trend of similarities of each slice \n",
        "  lines = []\n",
        "  for entity in similar:\n",
        "    '''\n",
        "    labelName = entity\n",
        "    if labelName in nameDict:\n",
        "      labelName = nameDict[labelName]\n",
        "    '''\n",
        "    line, = ax.plot(similar[entity], label = entity)\n",
        "    lines.append(line)\n",
        "  \n",
        "  ax.set_title('Slice ' + year)\n",
        "  ax.set_ylim(bottom = 0, top = 1)\n",
        "\n",
        "  plt.sca(ax)\n",
        "  plt.xticks((0, 1, 2, 3), groups)\n",
        "  \n",
        "  #plt.subplots_adjust(left = 0.1, right = 0.11)\n",
        "  '''\n",
        "  left  = 0.125,  # the left side of the subplots of the figure\n",
        "  right = 0.9,    # the right side of the subplots of the figure\n",
        "  bottom = 1,   # the bottom of the subplots of the figure\n",
        "  top = 0.9,      # the top of the subplots of the figure\n",
        "  wspace = 0.2,   # the amount of width reserved for blank space between subplots\n",
        "  hspace = 0.3)\n",
        "  '''\n",
        "  \n",
        "  names = []\n",
        "  for entity in similar.keys():\n",
        "    entity = entity.replace(\"DBRSTARTdbr:\", \"\").replace(\"DBREND\", \"\")\n",
        "    if entity in nameDict:\n",
        "      entity = nameDict[entity]\n",
        "    names.append(entity)\n",
        "\n",
        "  # Shrink current axis by 20%\n",
        "  box = ax.get_position()\n",
        "  ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "\n",
        "  # Put a legend to the right of the current axis\n",
        "  l = ax.legend(lines, names, \n",
        "            loc = legend_location, \n",
        "            bbox_to_anchor = legend_position)\n",
        "\n",
        "  #bbox = l.get_window_extent() \n",
        "  #print(bbox.width,bbox.height)\n",
        "\n",
        "def plot_trend_subplots(similarities, entity, subdirectory):\n",
        "  # create four polar axes, and access them through the returned array\n",
        "  trimmedEntity = entity_name.replace(\"DBRSTARTdbr:\", \"\").replace(\"DBREND\", \"\")\n",
        "  fig, axes = plt.subplots(2, 2, figsize = (12, 8), \n",
        "                           sharex = True, sharey = True, dpi = 300)\n",
        "  if trimmedEntity in nameDict:\n",
        "    trimmedEntity = nameDict[trimmedEntity]\n",
        "  plt.suptitle(\"Similarity trend of {}\".format(trimmedEntity), fontsize = 16)\n",
        "  for i, row in enumerate(axes):\n",
        "    for j, cell in enumerate(row):\n",
        "        if i == len(axes) - 1:\n",
        "            cell.set_xlabel(\"Time\")\n",
        "        if j == 0:\n",
        "            cell.set_ylabel(\"Similarity\")\n",
        "  subplot(similarities['1995'], axes[0, 0], '1995', (-0.2, 0.7), 0)\n",
        "  subplot(similarities['2000'], axes[0, 1], '2000', (1.1, 0.5), 6)\n",
        "  subplot(similarities['2005'], axes[1, 0], '2005', (-0.2, 0.7), 0)\n",
        "  subplot(similarities['2010'], axes[1, 1], '2010', (1.1, 0.5), 6)\n",
        "  \n",
        "  try:\n",
        "    os.mkdir(\"./images/trend/{}\".format(subdirectory))\n",
        "  except:\n",
        "    print(\"Directory already exist.\")\n",
        "  plt.savefig(\"./images/trend/{}/{}.png\".format(subdirectory, trimmedEntity), bbox_inches='tight')\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xFds6mWMkeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entity_name = \"DBRSTARTdbr:Al-QaedaDBREND\"\n",
        "n_similar = 3\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpcbpftsrws-",
        "colab_type": "text"
      },
      "source": [
        "# **PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWMlAO-srvJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_pca(model):\n",
        "  all_vectors = []\n",
        "  vocaboli = []\n",
        "  for key in model.wv.vocab:  # search for all words in the vocabulary and add the associated representation\n",
        "    vector = model.wv[key]\n",
        "    all_vectors.append(vector)\n",
        "    vocaboli.append(key)\n",
        "\n",
        "  pca = decomposition.PCA(n_components = 2, random_state = 42)\n",
        "  pca.fit(all_vectors)\n",
        "  pca_vectors = pca.transform(all_vectors)\n",
        "  pca_dictionary = dict(zip(vocaboli, pca_vectors))\n",
        "\n",
        "  return pca_dictionary\n",
        "\n",
        "\n",
        "pca_1995 = calculate_pca(model1995)\n",
        "pca_2000 = calculate_pca(model2000)\n",
        "pca_2005 = calculate_pca(model2005)\n",
        "pca_2010 = calculate_pca(model2010)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS9DJn5c2vDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_evolution_NN_entity(entity, pcas, models, colors, subdirectory, \n",
        "                             n_subplots = 4, n_most_similar = 5):\n",
        "  '''\n",
        "  Parameters:\n",
        "  entity: string containing the name of the entity to explore.\n",
        "  pcas: dict, key is the year and value is the actual pca of the alignment\n",
        "  models: dict key is the year and values is the model\n",
        "  n_subplots: number of subplots to be generated in the same figure\n",
        "  n_most_similar: number of most similar to find\n",
        "  '''\n",
        "  xs = dict() # key is the year, value is the list o x coord\n",
        "  ys = dict() # key is the year, value is the list o y coord\n",
        "  names = dict() # key is the year, value is the list o names to annotate\n",
        "  toPlot = dict() # key is the year, value is the list of flag of ax to plot,\n",
        "                  # in case of some exception thrown\n",
        "  for k in pcas.keys():\n",
        "    # The first element of the lists is always the entity coords\n",
        "    try:\n",
        "      x, y = pcas[k][entity]\n",
        "    except:\n",
        "      toPlot[k] = False\n",
        "      continue\n",
        "    toPlot[k] = True\n",
        "    xs[k] = [x]\n",
        "    ys[k] = [y]\n",
        "    names[k] = [entity]\n",
        "\n",
        "    # Adding the most similar\n",
        "    nameMostSimilar = [e[0] for e in models[k].most_similar(entity)[:n_most_similar]]\n",
        "\n",
        "    for name in nameMostSimilar:\n",
        "      x, y = pcas[k][name]\n",
        "      xs[k].append(x)\n",
        "      ys[k].append(y)\n",
        "    names[k] += nameMostSimilar\n",
        "\n",
        "    # cleaning the annotations from entities\n",
        "    names[k] = [e.replace(\"DBRSTARTdbr:\", '') for e in names[k]]\n",
        "    names[k] = [e.replace(\"DBREND\", '') for e in names[k]]\n",
        "\n",
        "  # Plotting\n",
        "  fig, axes = plt.subplots(nrows = 2, ncols = n_subplots // 2 + n_subplots % 2, \n",
        "                           sharex = False, sharey = False, \n",
        "                           figsize = (8, 6), dpi = 300)\n",
        "  # if we share the axes, the points will collapse showing that the entity \n",
        "  # semantic may have shifted (as in case of AppleInc) but messing a lot\n",
        "  # the annotation\n",
        "  trimmedEntity = entity.replace(\"DBRSTARTdbr:\", '').replace(\"DBREND\", '')\n",
        "  if trimmedEntity in nameDict:\n",
        "    trimmedEntity = nameDict[trimmedEntity]\n",
        "  plt.suptitle('Evolution of {}'.format(trimmedEntity), fontsize = 16)\n",
        "  for ax, k, c in zip(axes.reshape(-1), toPlot.keys(), colors[:n_subplots]):\n",
        "    ax.set_title(str(k), fontsize = 12)\n",
        "    if toPlot[k]:\n",
        "      ax.scatter(xs[k][0], ys[k][0], s = 100, color = c, marker = 'x')\n",
        "      ax.scatter(xs[k][1:], ys[k][1:], s = 40, color = c)\n",
        "      annotations = [ax.text(x, y, nameDict[name] if name in nameDict else name, fontsize = 10) for x, y, name in zip(xs[k][1:], ys[k][1:], names[k][1:])]\n",
        "      adjust_text(annotations, x = xs[k], y = ys[k], ax = ax)\n",
        "    ax.tick_params(axis = 'both', which = 'both', bottom = False, top = False,\n",
        "                   labelbottom = False, left = False, labelleft = False)\n",
        "\n",
        "  try:\n",
        "    os.mkdir(\"./images/evolution/{}\".format(subdirectory))\n",
        "  except:\n",
        "    pass\n",
        "  plt.savefig('./images/evolution/{}/{}.png'.format(subdirectory, trimmedEntity))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BKhsl2F23MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:GoogleDBREND', pcas, models, colors, subdirectory = 'test')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:AppleIncDBREND', pcas, models, colors, subdirectory = 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vo49OT76Y66",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def plotPCAovertime(entities, plotname, colors, subdirectory):\n",
        "  # entities: list of words to plot \n",
        "  data_1995 = list()\n",
        "  data_2000 = list()\n",
        "  data_2005 = list()\n",
        "  data_2010 = list()\n",
        "\n",
        "  markers = [\"o\", \"X\", \"s\", \"d\", \"P\", \"v\", \"|\", \"*\", \"_\"]\n",
        "  groups = (\"1995\", \"2000\", \"2005\", \"2010\")\n",
        "  names = list()\n",
        "  placeholder = np.array([\"?\", \"?\"])\n",
        "\n",
        "  for entity in entities:\n",
        "    if entity in pca_1995.keys():\n",
        "      data_1995 += [pca_1995[entity]] # add pca values\n",
        "    else:\n",
        "      print(\"Entity \" + str(entity) + \" missing in 1995\")\n",
        "      data_1995 += [placeholder]    # add placeholder for missing value\n",
        "    if entity in pca_2000.keys():\n",
        "      data_2000 += [pca_2000[entity]]\n",
        "    else:\n",
        "      print(\"Entity \" + str(entity) + \"  missing in 2000\")\n",
        "      data_2000 += [placeholder] \n",
        "    if entity in pca_2005.keys():\n",
        "      data_2005 += [pca_2005[entity]]\n",
        "    else:\n",
        "      print(\"Entity \" + str(entity) + \" missing in 2005\")\n",
        "      data_2005 += [placeholder] \n",
        "    if entity in pca_2010.keys():\n",
        "      data_2010 += [pca_2010[entity]]\n",
        "    else:\n",
        "      print(\"Entity \" + str(entity) + \" missing in 2010\")\n",
        "      data_2010 += [placeholder] \n",
        "    \n",
        "    name = entity                             # get name and remove annotation\n",
        "    name = name.replace(\"DBRSTARTdbr:\", \"\")\n",
        "    name = name.replace(\"DBREND\", \"\")\n",
        "    names.append(name)\n",
        "\n",
        "  fig = plt.figure(figsize = (8, 6), dpi = 300)\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  \n",
        "  dataScatter = (data_1995, data_2000, data_2005, data_2010)\n",
        "\n",
        "  for dataScatter, color in zip(dataScatter, colors):\n",
        "    i = 0\n",
        "    for point in dataScatter:\n",
        "      name = names[i]\n",
        "      x, y = point\n",
        "      if x != \"?\" and y != \"?\":   # check for missing value\n",
        "        ax.scatter(x, y, alpha = 0.9, c = color, edgecolors = 'none', s = 50, \n",
        "                   marker = markers[i], label = name)\n",
        "        #ax.annotate(name, (x, y))\n",
        "      i = i + 1 \n",
        "\n",
        "  plt.title('Evolution of entities over time', fontsize = 18)\n",
        "  plt.xlabel(\"PCA x value\", fontsize = 14)\n",
        "  plt.ylabel(\"PCA y value\", fontsize = 14)\n",
        "  plt.xticks(fontsize = 8)\n",
        "  plt.yticks(fontsize = 8)\n",
        "\n",
        "  # add legend\n",
        "  patches = list()\n",
        "  patch_1995 = mpatches.Patch(color = colors[0], label = '1995')\n",
        "  patch_2000 = mpatches.Patch(color = colors[1], label = '2000')\n",
        "  patch_2005 = mpatches.Patch(color = colors[2], label = '2005')\n",
        "  patch_2010 = mpatches.Patch(color = colors[3], label = '2010')\n",
        "\n",
        "  patches = [patch_1995, patch_2000, patch_2005, patch_2010]\n",
        "\n",
        "  i = 0\n",
        "  for n in names:\n",
        "    name = n\n",
        "    if name in nameDict:\n",
        "      name = nameDict[name]\n",
        "    tmp, = plt.plot([], [], marker = markers[i], ls = \"\", label = name, \n",
        "                    mfc = 'none', alpha = .9, color = 'Black', markersize = 9)\n",
        "    patches.append(tmp)\n",
        "    i = i + 1 \n",
        "    \n",
        "  # Shrink current axis by 20%\n",
        "  box = ax.get_position()\n",
        "  ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "\n",
        "  # Put a legend to the right of the current axis\n",
        "  ax.legend(handles = patches, loc = 'center left', bbox_to_anchor = (1, 0.5))\n",
        "  try:\n",
        "    os.mkdir(\"./images/pcaovertime/{}\".format(subdirectory))\n",
        "  except:\n",
        "    pass\n",
        "  plt.savefig(\"./images/pcaovertime/{}/{}.png\".format(subdirectory, plotname), \n",
        "              bbox_inches = 'tight')\n",
        "  plt.close()\n",
        "\n",
        "# Test\n",
        "entities = ['DBRSTARTdbr:MicrosoftDBREND', 'DBRSTARTdbr:AppleIncDBREND', \n",
        "            'DBRSTARTdbr:IBMDBREND', 'DBRSTARTdbr:GoogleDBREND']\n",
        "plotPCAovertime(entities, \"Apple\", colors, subdirectory = 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Fk4wCEXpRe",
        "colab_type": "text"
      },
      "source": [
        "# **Yugoslav Wars (1991 - 2001)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l_6HrYBXspn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''ENTITA':\n",
        "DBRSTARTdbr:SerbiaDBREND\n",
        "DBRSTARTdbr:CroatiaDBREND\n",
        "DBRSTARTdbr:KosovoDBREND             \n",
        "DBRSTARTdbr:WorldWarIIDBREND          \n",
        "DBRSTARTdbr:RussiaDBREND              \n",
        "DBRSTARTdbr:NATODBREND\n",
        "DBRSTARTdbr:BosniaandHerzegovinaDBREND\n",
        "\n",
        "\n",
        "#Aggiunti\n",
        "DBRSTARTdbr:YugoslaviaDBREND\n",
        "DBRSTARTdbr:BosnianWarDBREND\n",
        "DBRSTARTdbr:CivilwarDBREND\n",
        "DBRSTARTdbr:NazionalismDBREND\n",
        "DBRSTARTdbr:IndependenceDBREND       \n",
        "\n",
        "DBRSTARTdbr:SloveniaDBREND              \n",
        "DBRSTARTdbr:RepublicofMacedoniaDBREND  \n",
        "\n",
        "DBRSTARTdbr:SocialismDBREND             \n",
        "\n",
        "#Parole importanti\n",
        "#  war\n",
        "#  communism   DBRSTARTdbr:CommunismDBREND\n",
        "#  Genocide    DBRSTARTdbr:GenocideDBREND\n",
        "'''\n",
        "\n",
        "# CIVIL WAR  \n",
        "entity_name = 'DBRSTARTdbr:CivilwarDBREND'     \n",
        "n_similar = 5\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'YugoslavWars')\n",
        "\n",
        "\n",
        "# GENOCIDIO \n",
        "entity_name = 'DBRSTARTdbr:GenocideDBREND'     \n",
        "n_similar = 5                                 \n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'YugoslavWars')\n",
        "\n",
        "\n",
        "\n",
        "entities = ['DBRSTARTdbr:SerbiaDBREND', 'DBRSTARTdbr:KosovoDBREND', 'DBRSTARTdbr:BosniaandHerzegovinaDBREND', 'DBRSTARTdbr:YugoslaviaDBREND', 'DBRSTARTdbr:CroatiaDBREND','DBRSTARTdbr:CommunismDBREND', 'DBRSTARTdbr:BosnianWarDBREND', 'DBRSTARTdbr:NATODBREND', 'DBRSTARTdbr:CivilwarDBREND']\n",
        "plotPCAovertime(entities, \"comunism\", colors, subdirectory = 'YugoslavWars')\n",
        "\n",
        "\n",
        "\n",
        "entities = ['DBRSTARTdbr:SerbiaDBREND', 'DBRSTARTdbr:IndependenceDBREND', 'DBRSTARTdbr:BosniaandHerzegovinaDBREND', 'DBRSTARTdbr:SocialismDBREND', 'DBRSTARTdbr:BosnianWarDBREND', 'DBRSTARTdbr:CivilwarDBREND', 'DBRSTARTdbr:NationalismDBREND', 'war', 'DBRSTARTdbr:GenocideDBREND']\n",
        "plotPCAovertime(entities, \"war\", colors, subdirectory = 'YugoslavWars')\n",
        "\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "\n",
        "\n",
        "plot_evolution_NN_entity('war', pcas, models, colors, subdirectory = 'YugoslavWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:BosnianWarDBREND', pcas, models, colors, subdirectory = 'YugoslavWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:CivilwarDBREND', pcas, models, colors, subdirectory = 'YugoslavWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:NationalismDBREND', pcas, models, colors, subdirectory = 'YugoslavWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:GenocideDBREND', pcas, models, colors, subdirectory = 'YugoslavWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:YugoslaviaDBREND', pcas, models, colors, subdirectory = 'YugoslavWars')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4y8FEW8Q4gK",
        "colab_type": "text"
      },
      "source": [
        "# **Star wars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxj3yo3rQ8M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "'''ENTITA':\n",
        "DBRSTARTdbr:StarWarsDBREND o \n",
        "DBRSTARTdbr:StarWarsfilmDBREND # meglio usare starwars\n",
        "DBRSTARTdbr:StarTrekDBREND\n",
        "\n",
        "DBRSTARTdbr:SpecialeffectDBREND\n",
        "\n",
        "\n",
        "personaggi non ci sono in alcuni anni\n",
        "DBRSTARTdbr:LukeSkywalkerDBREND\n",
        "DBRSTARTdbr:Obi-WanKenobiDBREND \n",
        "DBRSTARTdbr:PrincessLeiaDBREND \n",
        "DBRSTARTdbr:HanSoloDBREND  \n",
        "\n",
        "# Parole importanti\n",
        "Star wars\n",
        "Phantom menace \n",
        "attack of clones\n",
        "revenge of the sith\n",
        "star trek\n",
        "film\n",
        "trilogy\n",
        "'''\n",
        "\n",
        "\n",
        "entity_name = 'DBRSTARTdbr:StarWarsDBREND'\n",
        "n_similar = 5\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'StarWars')\n",
        "\n",
        "\n",
        "\n",
        "entities = ['DBRSTARTdbr:StarWarsDBREND', 'DBRSTARTdbr:SpecialeffectDBREND', 'DBRSTARTdbr:StarTrekDBREND','film', 'trilogy','DBRSTARTdbr:HanSoloDBREND']\n",
        "plotPCAovertime(entities, \"comunism\", colors, subdirectory = 'StarWars')\n",
        "\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "\n",
        "\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:StarWarsDBREND', pcas, models, colors, subdirectory = 'StarWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:SpecialeffectDBREND', pcas, models, colors, subdirectory = 'StarWars')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:StarTrekDBREND', pcas, models, colors, subdirectory = 'StarWars')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSKr2hhJfKER",
        "colab_type": "text"
      },
      "source": [
        "# **Call of Duty**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht49smMofN6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "'''\n",
        "dbr:CallofDuty\n",
        "violence\n",
        "videogames\n",
        "terrorism\n",
        "war\n",
        "\n",
        "DBRSTARTdbr:VideoGameDBREND non c'è \n",
        "DBRSTARTdbr:GameDBREND\n",
        "'''\n",
        "\n",
        "\n",
        "entity_name = 'DBRSTARTdbr:CallofDutyDBREND'\n",
        "n_similar = 5\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'CallofDuty')\n",
        "\n",
        "\n",
        "\n",
        "entities = ['DBRSTARTdbr:CallofDutyDBREND','violence', 'DBRSTARTdbr:GameDBREND', 'war','terrorism', 'video', 'games']\n",
        "plotPCAovertime(entities, \"videogames\", colors, subdirectory = 'CallofDuty')\n",
        "\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:CallofDutyDBREND', pcas, models, colors, subdirectory = 'CallofDuty')\n",
        "plot_evolution_NN_entity('violence', pcas, models, colors, subdirectory = 'CallofDuty')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:GameDBREND', pcas, models, colors, subdirectory = 'CallofDuty')\n",
        "plot_evolution_NN_entity('war', pcas, models, colors, subdirectory = 'CallofDuty')\n",
        "plot_evolution_NN_entity('terrorism', pcas, models, colors, subdirectory = 'CallofDuty')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9gxB_Hwj6rp",
        "colab_type": "text"
      },
      "source": [
        "# **Eventi sportivi**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUIm8x8DkJb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "'''\n",
        "#squadre\n",
        "'DBRSTARTdbr:AmericanFootballTeamDBREND', \n",
        "'DBRSTARTdbr:ItalynationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:FrancenationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:GermanynationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:SpainnationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:NetherlandsnationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:BrazilnationalfootballteamDBREND' \n",
        "\n",
        "#paesi ospitanti\n",
        "# usa 94            DBRSTARTdbr:1994FIFAWorldCupDBREND     \n",
        "# francia 98        DBRSTARTdbr:1998FIFAWorldCupDBREND\n",
        "# corea giappone    DBRSTARTdbr:2002FIFAWorldCupDBREND\n",
        "# germania          DBRSTARTdbr:2006FIFAWorldCupDBREND\n",
        "# sud africa        DBRSTARTdbr:2010FIFAWorldCupDBREND\n",
        "\n",
        "'''\n",
        "\n",
        "entity_name = 'DBRSTARTdbr:AmericanFootballTeamDBREND'\n",
        "n_similar = 5\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'FIFA_World_Cup')\n",
        "\n",
        "\n",
        "\n",
        "entities = ['DBRSTARTdbr:AmericanFootballTeamDBREND', \n",
        "'DBRSTARTdbr:ItalynationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:FrancenationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:GermanynationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:SpainnationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:NetherlandsnationalfootballteamDBREND',  \n",
        "'DBRSTARTdbr:BrazilnationalfootballteamDBREND']\n",
        "plotPCAovertime(entities, \"football\", colors, subdirectory = 'WorlCup')\n",
        "\n",
        "\n",
        "'''\n",
        "# Olympic_Games\n",
        "entity_name = 'DBRSTARTdbr:OlympicGamesDBREND'\n",
        "n_similar = 5\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'Olympic_Games')\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyRjCpZZx0v",
        "colab_type": "text"
      },
      "source": [
        "# **Guerra Iraq (2003 - 2011)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-ZWiYafZybv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Entità:\n",
        "\n",
        "DBRSTARTdbr:GeorgeWBushDBREND\n",
        "DBRSTARTdbr:SaddamHusseinDBREND\n",
        "DBRSTARTdbr:NATODBREND\n",
        "DBRSTARTdbr:Al-QaedaDBREND\n",
        "DBRSTARTdbr:WorldTradeCenter1973E280932001DBREND\n",
        "DBRSTARTdbr:IraqDBREND\n",
        "DBRSTARTdbr:UnitedStatesDBREND\n",
        "DBRSTARTdbr:CentralIntelligenceAgencyDBREND\n",
        "DBRSTARTdbr:BillClintonDBREND\n",
        "DBRSTARTdbr:BarackObamaDBREND\n",
        "DBRSTARTdbr:PetroleumDBREND\n",
        "\n",
        "#Parole importanti\n",
        "# war\n",
        "# terrorism\n",
        "# oil \n",
        "\n",
        "'''\n",
        "\n",
        "keywords = [\"DBRSTARTdbr:GeorgeWBushDBREND\", \"DBRSTARTdbr:SaddamHusseinDBREND\",\n",
        "            \"DBRSTARTdbr:NATODBREND\", \"DBRSTARTdbr:Al-QaedaDBREND\",\n",
        "            \"DBRSTARTdbr:WorldTradeCenter1973E280932001DBREND\", \n",
        "            \"DBRSTARTdbr:IraqDBREND\", \"DBRSTARTdbr:UnitedStatesDBREND\", \n",
        "            \"DBRSTARTdbr:CentralIntelligenceAgencyDBREND\", \n",
        "            \"DBRSTARTdbr:BillClintonDBREND\", \"DBRSTARTdbr:BarackObamaDBREND\",\n",
        "            \"DBRSTARTdbr:PetroleumDBREND\", \"war\", \"terrorism\", \"oil\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6SqdvQnwShy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting trend subplots\n",
        "for kw in tqdm(keywords):\n",
        "  entity_name = kw\n",
        "  n_similar = 5\n",
        "\n",
        "  models = {}\n",
        "  models['1995'] = model1995\n",
        "  models['2000'] = model2000\n",
        "  models['2005'] = model2005\n",
        "  models['2010'] = model2010\n",
        "\n",
        "  similarities = {}\n",
        "\n",
        "  similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "  similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "  similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "  similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "  plot_trend_subplots(similarities, entity_name, subdirectory = 'IraqWar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tkGJ9Ew_JBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting evolution\n",
        "for kw in tqdm(keywords):\n",
        "  pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "  models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "  plot_evolution_NN_entity(kw, pcas, models, colors, subdirectory = 'IraqWar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBByzIzQAa79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting overtime\n",
        "chiefs = [\"DBRSTARTdbr:GeorgeWBushDBREND\", \"DBRSTARTdbr:SaddamHusseinDBREND\",\n",
        "          \"DBRSTARTdbr:Al-QaedaDBREND\",  \"DBRSTARTdbr:BillClintonDBREND\", \n",
        "          \"DBRSTARTdbr:PetroleumDBREND\", \"war\", \"terrorism\"]\n",
        "plotPCAovertime(chiefs, \"IraqWar-Chiefs\", colors, subdirectory = 'IraqWar')\n",
        "\n",
        "nine_eleven = [\"DBRSTARTdbr:SaddamHusseinDBREND\",\n",
        "               \"DBRSTARTdbr:Al-QaedaDBREND\", \"DBRSTARTdbr:WorldTradeCenter1973E280932001DBREND\", \n",
        "               \"DBRSTARTdbr:CentralIntelligenceAgencyDBREND\", \"terrorism\"]\n",
        "plotPCAovertime(nine_eleven, \"IraqWar-9_11\", colors, subdirectory = 'IraqWar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO6fucw_m3NP",
        "colab_type": "text"
      },
      "source": [
        "# **Economical crisis (2008)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AXNEtt2jSrw1",
        "colab": {}
      },
      "source": [
        "'''Entità:\n",
        "DBRSTARTdbr:LehmanBrothersDBREND\n",
        "DBRSTARTdbr:HenryPaulsonDBREND   no 2000\n",
        "DBRSTARTdbr:UnitedStatesCongressDBREND\n",
        "DBRSTARTdbr:StockmarketDBREND\n",
        "DBRSTARTdbr:StockDBREND\n",
        "DBRSTARTdbr:MarketeconomicsDBREND\n",
        "DBRSTARTdbr:FinancialmarketDBREND\n",
        "\n",
        "#Parole di interesse\n",
        "#economy\n",
        "#crisis\n",
        "#debt\n",
        "#congress\n",
        "#stock market (non la trovo)\n",
        "#stock\n",
        "#stocks\n",
        "#Soros\n",
        "'''\n",
        "folder = 'economical_crisis'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zalKwmYZYah-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA over time plots\n",
        "\n",
        "entities = ['economy', 'crisis', 'DBRSTARTdbr:LehmanBrothersDBREND', 'DBRSTARTdbr:HenryPaulsonDBREND']\n",
        "plotPCAovertime(entities, folder, colors, folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGmo4hGPcNOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities = ['dbr:LehmanBrothers', 'dbr:HenryPaulson', 'dbr:UnitedStatesCongress', 'dbr:Stockmarket', 'dbr:Stock', 'dbr:Marketeconomics', 'dbr:Financialmarket']\n",
        "for i in range(len(entities)):\n",
        "  entities[i] = \"DBRSTART\" + entities[i] + 'DBREND'\n",
        "\n",
        "words = ['economy', 'crisis', 'debt', 'congress', 'stock', 'Soros']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsbFXArgYjrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot evolution\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "\n",
        "for entity in entities:\n",
        "  plot_evolution_NN_entity(entity, pcas, models, colors, subdirectory = folder)\n",
        "\n",
        "for word in words:\n",
        "  plot_evolution_NN_entity(word, pcas, models, colors, subdirectory = folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ5NQm7hYW4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting trend subplots\n",
        "for kw in tqdm(entities):\n",
        "  entity_name = kw\n",
        "  n_similar = 5\n",
        "\n",
        "  models = {}\n",
        "  models['1995'] = model1995\n",
        "  models['2000'] = model2000\n",
        "  models['2005'] = model2005\n",
        "  models['2010'] = model2010\n",
        "\n",
        "  similarities = {}\n",
        "\n",
        "  similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "  similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "  similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "  similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "  plot_trend_subplots(similarities, entity_name, subdirectory = 'economical_crisis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP04oOqmqEqN",
        "colab_type": "text"
      },
      "source": [
        "# **Russia**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tshQ3fOsJXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Entità:\n",
        "DBRSTARTdbr:VladimirPutinDBREND\n",
        "DBRSTARTdbr:BillClintonDBREND\n",
        "DBRSTARTdbr:KGBDBREND\n",
        "DBRSTARTdbr:CentralIntelligenceAgencyDBREND\n",
        "DBRSTARTdbr:FederalSecurityServiceDBREND\n",
        "DBRSTARTdbr:ChechnyaDBREND\n",
        "DBRSTARTdbr:CommunismDBREND\n",
        "DBRSTARTdbr:CrimeaDBREND\n",
        "DBRSTARTdbr:CrimeanWarDBREND\n",
        "DBRSTARTdbr:DmitryMedvedevDBREND\n",
        "#Parole di interesse\n",
        "#comunism #QUESTA PAROLA MANCA ma c'è entità \n",
        "#war\n",
        "nuclear\n",
        "'''\n",
        "keywords = [\"DBRSTARTdbr:VladimirPutinDBREND\", \"DBRSTARTdbr:BillClintonDBREND\",\n",
        "            \"DBRSTARTdbr:KGBDBREND\", \"DBRSTARTdbr:CentralIntelligenceAgencyDBREND\",\n",
        "            \"DBRSTARTdbr:FederalSecurityServiceDBREND\", \"DBRSTARTdbr:ChechnyaDBREND\",\n",
        "            \"DBRSTARTdbr:CommunismDBREND\", \"DBRSTARTdbr:CrimeaDBREND\", \n",
        "            \"DBRSTARTdbr:CrimeanWarDBREND\", \"war\", \"nuclear\", \n",
        "            \"DBRSTARTdbr:DmitryMedvedevDBREND\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zpQrRQICJ1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting trend subplots\n",
        "for kw in tqdm(keywords):\n",
        "  entity_name = kw\n",
        "  n_similar = 5\n",
        "\n",
        "  models = {}\n",
        "  models['1995'] = model1995\n",
        "  models['2000'] = model2000\n",
        "  models['2005'] = model2005\n",
        "  models['2010'] = model2010\n",
        "\n",
        "  similarities = {}\n",
        "\n",
        "  similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "  similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "  similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "  similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "  plot_trend_subplots(similarities, entity_name, subdirectory = 'Russia')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F3tHjjMCNyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting evolution\n",
        "for kw in tqdm(keywords):\n",
        "  pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "  models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "  plot_evolution_NN_entity(kw, pcas, models, colors, subdirectory = 'Russia')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgeeMAn0CSkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting overtime\n",
        "chiefs_war = [\"DBRSTARTdbr:VladimirPutinDBREND\", \"DBRSTARTdbr:BillClintonDBREND\",\n",
        "              \"war\", \"nuclear\", \"DBRSTARTdbr:DmitryMedvedevDBREND\"]\n",
        "plotPCAovertime(chiefs_war, \"Russia-Chiefs & war\", colors, subdirectory = 'Russia')\n",
        "\n",
        "agencies = [\"DBRSTARTdbr:KGBDBREND\", \"DBRSTARTdbr:CentralIntelligenceAgencyDBREND\",\n",
        "            \"DBRSTARTdbr:FederalSecurityServiceDBREND\", \"DBRSTARTdbr:ChechnyaDBREND\",\n",
        "            \"nuclear\"]\n",
        "plotPCAovertime(agencies, \"Russia-agencies\", colors, subdirectory = 'Russia')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb4WXcY8jaVy",
        "colab_type": "text"
      },
      "source": [
        "# **Artificial Intelligence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmvGo2YSjhAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Entità\n",
        "DBRSTARTdbr:GoogleDBREND\n",
        "DBRSTARTdbr:FacebookDBREND\n",
        "DBRSTARTdbr:MassachusettsInstituteofTechnologyDBREND\n",
        "DBRSTARTdbr:ArtificialneuralnetworkDBREND\n",
        "DBRSTARTdbr:BiologicalneuralnetworkDBREND (c'è nei file ma non nei modelli)\n",
        "DBRSTARTdbr:GenomicsDBREND\n",
        "\n",
        "DBRSTARTdbr:ComputerprogrammingDBREND\n",
        "DBRSTARTdbr:OpenInventionNetworkDBREND\n",
        "\n",
        "#Parole di interesse\n",
        "'''\n",
        "folder = 'artificial_intelligence'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ167HZyuTjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA over time plots\n",
        "\n",
        "entities = ['Google', 'Facebook', 'MassachusettsInstituteofTechnology', 'Artificialneuralnetwork', 'Genomics', 'Computerprogramming']\n",
        "for i in range(len(entities)):\n",
        "  entities[i] = \"DBRSTARTdbr:\" + entities[i] + 'DBREND'\n",
        "\n",
        "plotPCAovertime(entities, folder, colors, folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd0jHf3tjqpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities = ['Google', 'Facebook', 'MassachusettsInstituteofTechnology', 'Artificialneuralnetwork', 'Genomics', 'Computerprogramming']\n",
        "for i in range(len(entities)):\n",
        "  entities[i] = \"DBRSTARTdbr:\" + entities[i] + 'DBREND'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjOVNsttkBh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot evolution\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "\n",
        "for entity in entities:\n",
        "  plot_evolution_NN_entity(entity, pcas, models, colors, subdirectory = folder)\n",
        "\n",
        "for word in words:\n",
        "  plot_evolution_NN_entity(word, pcas, models, colors, subdirectory = folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVl1xsgekBQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting trend subplots\n",
        "for kw in tqdm(entities):\n",
        "  entity_name = kw\n",
        "  n_similar = 5\n",
        "\n",
        "  models = {}\n",
        "  models['1995'] = model1995\n",
        "  models['2000'] = model2000\n",
        "  models['2005'] = model2005\n",
        "  models['2010'] = model2010\n",
        "\n",
        "  similarities = {}\n",
        "\n",
        "  similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "  similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "  similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "  similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "  plot_trend_subplots(similarities, entity_name, subdirectory = 'artificial_intelligence')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFjzHF--o459",
        "colab_type": "text"
      },
      "source": [
        "# **Harry Potter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vz1gCxao_Wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Entità\n",
        "DBRSTARTdbr:JKRowlingDBREND\n",
        "DBRSTARTdbr:HarryPotterDBREND\n",
        "DBRSTARTdbr:LordVoldemortDBREND\n",
        "\n",
        "Parole di interesse\n",
        "\n",
        "'''\n",
        "\n",
        "folder = 'harry_potter'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVyRU5E-tkRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA over time plots\n",
        "\n",
        "entities = ['JKRowling', 'HarryPotter', 'LordVoldemort']\n",
        "for i in range(len(entities)):\n",
        "  entities[i] = \"DBRSTARTdbr:\" + entities[i] + 'DBREND'\n",
        "\n",
        "plotPCAovertime(entities, folder, colors, folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R87pBR4pCB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities = ['JKRowling', 'HarryPotter', 'LordVoldemort']\n",
        "for i in range(len(entities)):\n",
        "  entities[i] = \"DBRSTARTdbr:\" + entities[i] + 'DBREND'\n",
        "\n",
        "words = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhBgV77mpFBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = 'DBRSTARTdbr:LordVoldemortDBREND'\n",
        "if word in models['1995']: \n",
        "  print(models['1995'].most_similar(word)[:5])\n",
        "if word in models['2000']: \n",
        "  print(models['2000'].most_similar(word)[:5])\n",
        "if word in models['2005']: \n",
        "  print(models['2005'].most_similar(word)[:5])\n",
        "if word in models['2010']: \n",
        "  print(models['2010'].most_similar(word)[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1YA-iTPo_HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot evolution\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "\n",
        "for entity in entities:\n",
        "  plot_evolution_NN_entity(entity, pcas, models, colors, subdirectory = folder)\n",
        "\n",
        "for word in words:\n",
        "  plot_evolution_NN_entity(word, pcas, models, colors, subdirectory = folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUrg138po-1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting trend subplots\n",
        "for kw in tqdm(entities):\n",
        "  entity_name = kw\n",
        "  n_similar = 5\n",
        "\n",
        "  models = {}\n",
        "  models['1995'] = model1995\n",
        "  models['2000'] = model2000\n",
        "  models['2005'] = model2005\n",
        "  models['2010'] = model2010\n",
        "\n",
        "  similarities = {}\n",
        "\n",
        "  similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "  similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "  similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "  similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "  plot_trend_subplots(similarities, entity_name, subdirectory = 'harry_potter')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obCxFFUXN_M_",
        "colab_type": "text"
      },
      "source": [
        "# **More analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM1bR9Ut3HQN",
        "colab_type": "text"
      },
      "source": [
        "# Occurancies Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOrSoudx2-2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count occurancies in evry year and return also the total count\n",
        "def countOccorrunces(word):\n",
        "  filenames = ['/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_1995.txt', '/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2000.txt', '/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2005.txt', '/content/gdrive/My Drive/ProgettoAI/Merged_articles/Merged_2010.txt']\n",
        "  counts = list()\n",
        "  for file in filenames:\n",
        "    data = []\n",
        "    with open(file, \"r\") as fifi:\n",
        "      data = fifi.readlines()\n",
        "      counts.append(data[0].count(word))\n",
        "  return counts, sum(counts)\n",
        "\n",
        "\n",
        "# print occurancies calculate from the function \"countOccorrunces\"\n",
        "def printOccurancies(entities):\n",
        "  for wordToCount in entities:\n",
        "    counts, total = countOccorrunces(wordToCount)\n",
        "    print(\"Occorrences over years \" + wordToCount + \": \", counts)\n",
        "    print(\"Total occurances \" + wordToCount + \": \", total)\n",
        "    print()\n",
        "\n",
        "\n",
        "# calculate cosine similarity for word and list of entity (entities) over years\n",
        "def calculateSimilarityOverYears(word, entities):\n",
        "  similarity= {}\n",
        "  for entity in entities:\n",
        "    tmp = []\n",
        "    if entity in model1995:\n",
        "      tmp.append(round(calculateSimiliraty(model1995[word], model1995[entity]), 3))\n",
        "    else:\n",
        "        tmp.append(0)\n",
        "    if entity in model2000:\n",
        "      tmp.append(round(calculateSimiliraty(model2000[word], model2000[entity]), 3))\n",
        "    else:\n",
        "        tmp.append(0)\n",
        "    if entity in model2005:\n",
        "      tmp.append(round(calculateSimiliraty(model2005[word], model2005[entity]), 3))\n",
        "    else:\n",
        "        tmp.append(0)\n",
        "    if entity in model2010:\n",
        "      tmp.append(round(calculateSimiliraty(model2010[word], model2010[entity]), 3))\n",
        "    else:\n",
        "        tmp.append(0)\n",
        "    similarity[entity] = tmp\n",
        "\n",
        "  return similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcgLtcxwKCLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "# frequency of Yugoslavia wars\n",
        "print(\"frequenza parole guerra Yugoslava\")\n",
        "entities = ['DBRSTARTdbr:CivilwarDBREND', 'DBRSTARTdbr:BosnianWarDBREND','DBRSTARTdbr:NationalismDBREND','DBRSTARTdbr:GenocideDBREND','DBRSTARTdbr:YugoslaviaDBREND','DBRSTARTdbr:NATODBREND']\n",
        "printOccurancies(entities)\n",
        "\n",
        "\n",
        "# Frequency of obama entity\n",
        "print(\"frequenza Obama: \", countOccorrunces('DBRSTARTdbr:BarackObamaDBREND') )\n",
        "\n",
        "\n",
        "print(\"frequenza  WorldTradeCenter \", countOccorrunces('WorldTradeCenter1973E280932001'))\n",
        "\n",
        "\n",
        "# Confronto petrolio e guerra \n",
        "print(\"Confronto petrolio e guerra: \")\n",
        "word = 'DBRSTARTdbr:PetroleumDBREND'\n",
        "entities = ['war', 'DBRSTARTdbr:IraqDBREND', 'DBRSTARTdbr:IranDBREND']\n",
        "results = calculateSimilarityOverYears(word, entities)\n",
        "print(results)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "# Similarità KGB con entità Iraq (Bin Laden, nuclear, saddam, Iraq ...)\n",
        "print(\"KGB con Iraq (Bin Laden, nuclear, saddam, Iraq) : \")\n",
        "word = 'DBRSTARTdbr:KGBDBREND'\n",
        "entities = [\"nuclear\", \"DBRSTARTdbr:IraqDBREND\", \"DBRSTARTdbr:IranDBREND\", \"DBRSTARTdbr:SaddamHusseinDBREND\", \"DBRSTARTdbr:OsamabinLadenDBREND\"]\n",
        "results = calculateSimilarityOverYears(word, entities)\n",
        "print(results)\n",
        "print()\n",
        "\n",
        "plotPCAovertime(entities, \"richiestaPapettiRussia\", colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "# Nato con nazioni tipo: Bosnia, Iraq e altri -> overtime\n",
        "entities = ['DBRSTARTdbr:KosovoDBREND', 'DBRSTARTdbr:BosniaandHerzegovinaDBREND', 'DBRSTARTdbr:YugoslaviaDBREND', 'DBRSTARTdbr:IraqDBREND','DBRSTARTdbr:IranDBREND', 'DBRSTARTdbr:NATODBREND']\n",
        "plotPCAovertime(entities, \"richiestaPape1\", colors, subdirectory = 'Papetti')\n",
        "\n",
        "# Overtime -> Bosnia e comunismo\n",
        "entities = ['DBRSTARTdbr:BosniaandHerzegovinaDBREND', 'DBRSTARTdbr:CommunismDBREND']\n",
        "plotPCAovertime(entities, \"richiestaPape2\", colors, subdirectory = 'Papetti')\n",
        "\n",
        "# Overtime ->  war, genocidio, nazionalismo e civilwar\n",
        "entities = ['war', 'DBRSTARTdbr:CivilwarDBREND', 'DBRSTARTdbr:NationalismDBREND', 'DBRSTARTdbr:GenocideDBREND']\n",
        "plotPCAovertime(entities, \"richiestaPape3\", colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "\n",
        "# indagare cosa c'è vicino alla rowling  DBRSTARTdbr:JKRowlingDBREND DBRSTARTdbr:HarryPotterDBREND\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:JKRowlingDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "\n",
        "# Frequency Harry Potter e rowling\n",
        "print(\"frequenza parole Harry Potter e rowling\")\n",
        "entities = ['DBRSTARTdbr:JKRowling', 'DBRSTARTdbr:HarryPotterDBREND']\n",
        "printOccurancies(entities)\n",
        "\n",
        "\n",
        "# Similarità harry e rowling\n",
        "print(\"Similarità harry e rowling: \")\n",
        "word = 'DBRSTARTdbr:HarryPotterDBREND'\n",
        "entities = ['DBRSTARTdbr:JKRowlingDBREND']\n",
        "results = calculateSimilarityOverYears(word, entities)\n",
        "print(results)\n",
        "print()\n",
        "\n",
        "# Più simili a Harry Potter\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:HarryPotterDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "\n",
        "# Spostamento università\n",
        "print(\"frequenza universities\")\n",
        "entities = ['DBRSTARTdbr:MassachusettsInstituteofTechnologyDBREND', 'DBRSTARTdbr:ColumbiaUniversityDBREND', 'DBRSTARTdbr:PrincetonUniversityDBREND', 'DBRSTARTdbr:NewYorkUniversityDBREND', 'DBRSTARTdbr:StanfordUniversityDBREND', 'DBRSTARTdbr:UniversityofCaliforniaBerkeleyDBREND', 'DBRSTARTdbr:UniversityofChicagoDBREND']\n",
        "printOccurancies(entities)\n",
        "\n",
        "\n",
        "entity_name = 'DBRSTARTdbr:MassachusettsInstituteofTechnologyDBREND'\n",
        "n_similar = 5\n",
        "\n",
        "models = {}\n",
        "models['1995'] = model1995\n",
        "models['2000'] = model2000\n",
        "models['2005'] = model2005\n",
        "models['2010'] = model2010\n",
        "\n",
        "similarities = {}\n",
        "\n",
        "similarities['1995'] = get_trend_single_slice(entity_name, n_similar, models, '1995')\n",
        "similarities['2000'] = get_trend_single_slice(entity_name, n_similar, models, '2000')\n",
        "similarities['2005'] = get_trend_single_slice(entity_name, n_similar, models, '2005')\n",
        "similarities['2010'] = get_trend_single_slice(entity_name, n_similar, models, '2010')\n",
        "\n",
        "plot_trend_subplots(similarities, entity_name, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "\n",
        "entities = ['DBRSTARTdbr:MassachusettsInstituteofTechnologyDBREND', 'DBRSTARTdbr:ColumbiaUniversityDBREND', 'DBRSTARTdbr:PrincetonUniversityDBREND', 'DBRSTARTdbr:NewYorkUniversityDBREND', 'DBRSTARTdbr:StanfordUniversityDBREND', 'DBRSTARTdbr:UniversityofCaliforniaBerkeleyDBREND', 'DBRSTARTdbr:UniversityofChicagoDBREND']\n",
        "plotPCAovertime(entities, \"university\", colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:MassachusettsInstituteofTechnologyDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:ColumbiaUniversityDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:PrincetonUniversityDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:NewYorkUniversityDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:StanfordUniversityDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:UniversityofCaliforniaBerkeleyDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:UniversityofChicagoDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "\n",
        "# Vedere AOL come si evolve Evolution\n",
        "print(\"frequenza AOL\")\n",
        "entities = ['DBRSTARTdbr:AOLDBREND', 'DBRSTARTdbr:GoogleDBREND']\n",
        "printOccurancies(entities)\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:AOLDBREND', pcas, models, colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "\n",
        "# Insieme di nazioni vedere come si evolvono overtime\n",
        "print(\"frequenza Nazioni\") #\"DBRSTARTdbr:NetherlandsDBREND\", \"DBRSTARTdbr:PolandDBREND\",\n",
        "entities = [\"DBRSTARTdbr:UnitedKingdomDBREND\", \"DBRSTARTdbr:AustraliaDBREND\", \"DBRSTARTdbr:IranDBREND\", \"DBRSTARTdbr:IraqDBREND\", \"DBRSTARTdbr:UnitedStatesDBREND\", \"DBRSTARTdbr:ItalyDBREND\", \"DBRSTARTdbr:GermanyDBREND\", \"DBRSTARTdbr:FranceDBREND\"]\n",
        "printOccurancies(entities)\n",
        "plotPCAovertime(entities, \"States\", colors, subdirectory = 'Papetti')\n",
        "\n",
        "\n",
        "entities = [\"DBRSTARTdbr:UnitedKingdomDBREND\", \"DBRSTARTdbr:AustraliaDBREND\", \"DBRSTARTdbr:IranDBREND\", \"DBRSTARTdbr:IraqDBREND\", \"DBRSTARTdbr:UnitedStatesDBREND\"]\n",
        "printOccurancies(entities)\n",
        "plotPCAovertime(entities, \"States2\", colors, subdirectory = 'Papetti')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3tQBy4R0de2",
        "colab_type": "text"
      },
      "source": [
        "# **LTN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4DbmVZf0ZiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### COSTANTI\n",
        "# introduciamo le costanti (TUTTE, anche quelle che non useremo per trainare le LTN le entità) che saranno rappresentate nello spazio\n",
        "# sintassi : ltnw.constant(nome, vettore_di_embedding)\n",
        "\n",
        "\n",
        "### PREDICATI\n",
        "# introduciamo qui i predicati(le reti neurali da addestrare)\n",
        "# sintassi : ltnw.predicate(nome, num_input * dim_embedding)\n",
        "\n",
        "\n",
        "### VARIABILI\n",
        "# disponiamo delle variabili utili campionando lo spazio. Possiamo definire il campionamento a mano o passare un dominio\n",
        "# sintassi :\n",
        "#ltnw.variable(\"?a\", np.random.uniform(min_dom, max_dom, (numero_campionamenti_spazio, embedding_dim)).astype(\"float32\"));\n",
        "\n",
        "\n",
        "### ASSIOMI\n",
        "# introduciamo cose che conosciamo\n",
        "# sintassi : \n",
        "#ltnw.axiom(predicato(costante)) \n",
        "#ltnw.axiom(~predicato(costante)) \n",
        "#ltnw.axiom(predicato1(costante1) | predicato2(costante2))\n",
        "\n",
        "\n",
        "### QUANTIFICATORI\n",
        "# quantificano le variabili (forall, exists)\n",
        "# sintassi : ltnw.axiom(\"forall ?a: Predicate(?a) -> ~Predicate(?a)\")\n",
        "\n",
        "\n",
        "### INIZIALIZZAZIONE\n",
        "#ltnw.initialize_knowledgebase(optimizer=tf.train.AdamOptimizer())\n",
        "#ltnw.train(max_epochs = 1000, track_sat_levels=100, sat_level_epsilon=.99)\n",
        "\n",
        "### QUERY\n",
        "# sintassi : \n",
        "#ltnw.ask(predicato(costante))\n",
        "#ltnw.ask(exists variabile: predicato(variabile))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClhD9Ges-nAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import models\n",
        "model1995 = Word2Vec.load(\"model/Merged_1995.model\")\n",
        "model2000 = Word2Vec.load(\"model/Merged_2000.model\")\n",
        "model2005 = Word2Vec.load(\"model/Merged_2005.model\")\n",
        "model2010 = Word2Vec.load(\"model/Merged_2010.model\")\n",
        "\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_1995\", 'rb')\n",
        "data_1995 = pickle.load(file)\n",
        "file.close()\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_2000\", 'rb')\n",
        "data_2000 = pickle.load(file)\n",
        "file.close()\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_2005\", 'rb')\n",
        "data_2005 = pickle.load(file)\n",
        "file.close()\n",
        "file = open(\"/content/gdrive/My Drive/ProgettoAI/Merged_articles/data_2010\", 'rb')\n",
        "data_2010 = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alFH8C-6k73g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to ignore irrelevant entities\n",
        "def extract_relevant_entities(data, embedding):\n",
        "  relevant_entities = list()\n",
        "  entities, _ = find_entities(data)\n",
        "  for entity in entities:\n",
        "    if entity in embedding:\n",
        "      relevant_entities.append(entity)\n",
        "  return relevant_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv4CPkk_NUvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rel_ent_1995 = extract_relevant_entities(data_1995, model1995)\n",
        "rel_ent_2000 = extract_relevant_entities(data_2000, model2000)\n",
        "rel_ent_2005 = extract_relevant_entities(data_2005, model2005)\n",
        "rel_ent_2010 = extract_relevant_entities(data_2010, model2010)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFu9I3Z-sRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set domain\n",
        "domain1995 = np.array(list(map(lambda x : x, model1995[rel_ent_1995])))\n",
        "domain2000 = np.array(list(map(lambda x : x, model2000[rel_ent_2000])))\n",
        "domain2005 = np.array(list(map(lambda x : x, model2005[rel_ent_2005])))\n",
        "domain2010 = np.array(list(map(lambda x : x, model2010[rel_ent_2010])))\n",
        "domain = np.concatenate((domain1995, domain2000, domain2005, domain2010))\n",
        "\n",
        "max_dom = np.amax(domain.flatten())\n",
        "min_dom = np.amin(domain.flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlWCZYjf0wMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean entities names\n",
        "def clean_entity_name(value):\n",
        "  value = value.replace(\":\", \"\")\n",
        "  # remove prefix and suffix\n",
        "  value = value[11:]\n",
        "  value = value[:-6]\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nCsQdsbt4LI",
        "colab_type": "text"
      },
      "source": [
        "## First batch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEiQmRfK2wOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltnw._reset()\n",
        "ltn_entities = set()\n",
        "\n",
        "# DEFINE COSTANTS\n",
        "# Need to load ALL entites that we will use, not only the ones uset for training the LTN\n",
        "for entity in rel_ent_1995:\n",
        "    ltnw.constant(\"{}1995\".format(clean_entity_name(entity)), model1995[entity])\n",
        "    ltn_entities.add(\"{}1995\".format(clean_entity_name(entity)))\n",
        "for entity in rel_ent_2000:\n",
        "    ltnw.constant(\"{}2000\".format(clean_entity_name(entity)), model2000[entity])\n",
        "    ltn_entities.add(\"{}2000\".format(clean_entity_name(entity)))\n",
        "for entity in rel_ent_2005:\n",
        "    ltnw.constant(\"{}2005\".format(clean_entity_name(entity)), model2005[entity])\n",
        "    ltn_entities.add(\"{}2005\".format(clean_entity_name(entity)))\n",
        "for entity in rel_ent_2010:\n",
        "    ltnw.constant(\"{}2010\".format(clean_entity_name(entity)), model2010[entity])\n",
        "    ltn_entities.add(\"{}2010\".format(clean_entity_name(entity)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h4DeqHja7qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE PREDICATE\n",
        "ltnw.predicate(\"State\", 1 * embedding_size)\n",
        "ltnw.predicate(\"Company\", 1 * embedding_size)\n",
        "ltnw.predicate(\"Actor\", 1 * embedding_size)\n",
        "ltnw.predicate(\"Human\", 1 * embedding_size)\n",
        "ltnw.predicate(\"BornIn\", 2 * embedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqtSnYV_MESg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE VARIABLES\n",
        "# 250 is a good compromise between time and performance\n",
        "ltnw.variable(\"?a\", np.random.uniform(min_dom, max_dom, (250, embedding_size)).astype(\"float32\"))\n",
        "ltnw.variable(\"?b\", np.random.uniform(min_dom, max_dom, (250, embedding_size)).astype(\"float32\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkUHVuRiMG1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE AXIOMS\n",
        "states = [\"UnitedKingdom\", \"Netherlands\", \"Poland\", \"Australia\", \"Iran\", \"Iraq\", \"UnitedStates\", \"Italy\", \"Germany\", \"France\"]\n",
        "companies = [\"AppleInc\", \"IBM\", \"Hewlett-Packard\", \"Dell\", \"Microsoft\", \"Amazoncom\", \"Intel\", \"AdvancedMicroDevices\"]\n",
        "actors = [\"MarlonBrando\", \"JackNicholson\", \"RobertDeNiro\", \"AlPacino\", \"DanielDay-Lewis\", \"DustinHoffman\", \"TomHanks\", \"AnthonyHopkins\"]\n",
        "for state in states:\n",
        "  for year in [\"1995\", \"2000\", \"2005\"]:\n",
        "    if state + year in ltn_entities:\n",
        "      ltnw.axiom(\"State({})\".format(state + year))\n",
        "      ltnw.axiom(\"~ Company({})\".format(state + year))\n",
        "      ltnw.axiom(\"~ Actor({})\".format(state + year))\n",
        "\n",
        "for company in companies:\n",
        "  for year in [\"1995\", \"2000\", \"2005\"]:\n",
        "    if company + year in ltn_entities:\n",
        "      ltnw.axiom(\"Company({})\".format(company + year))\n",
        "      ltnw.axiom(\"~ State({})\".format(company + year))\n",
        "      ltnw.axiom(\"~ Actor({})\".format(company + year))\n",
        "\n",
        "for actor in actors:\n",
        "  for year in [\"1995\", \"2000\", \"2005\"]:\n",
        "    if actor + year in ltn_entities:\n",
        "      ltnw.axiom(\"~ Company({})\".format(actor + year))\n",
        "      ltnw.axiom(\"~ State({})\".format(actor + year))\n",
        "      ltnw.axiom(\"Actor({})\".format(actor + year))\n",
        "\n",
        "ltnw.axiom(\"BornIn(MarlonBrando1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(MarlonBrando1995, Australia1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(MarlonBrando1995, Iran1995)\")\n",
        "ltnw.axiom(\"BornIn(JackNicholson1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(JackNicholson1995, France1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(JackNicholson1995, Iraq1995)\")\n",
        "ltnw.axiom(\"BornIn(RobertDeNiro1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(RobertDeNiro1995, SaudiArabia1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(RobertDeNiro1995, France1995)\")\n",
        "ltnw.axiom(\"BornIn(AlPacino1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(AlPacino1995, Netherlands1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(AlPacino1995, Poland1995)\")\n",
        "ltnw.axiom(\"BornIn(DanielDay-Lewis1995, UnitedKingdom1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(DanielDay-Lewis1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(DanielDay-Lewis1995, SaudiArabia1995)\")\n",
        "ltnw.axiom(\"BornIn(DustinHoffman1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(DustinHoffman1995, France1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(DustinHoffman1995, Iran1995)\")\n",
        "ltnw.axiom(\"BornIn(TomHanks1995, UnitedStates1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(TomHanks1995, Italy1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(TomHanks1995, Germany1995)\")\n",
        "ltnw.axiom(\"BornIn(AnthonyHopkins1995, UnitedKingdom1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(AnthonyHopkins1995, Italy1995)\")\n",
        "ltnw.axiom(\" ~ BornIn(AnthonyHopkins1995, Germany1995)\")\n",
        "ltnw.axiom(\"BornIn(MarlonBrando2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(MarlonBrando2000, Australia2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(MarlonBrando2000, Iran2000)\")\n",
        "ltnw.axiom(\"BornIn(JackNicholson2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(JackNicholson2000, France2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(JackNicholson2000, Iraq2000)\")\n",
        "ltnw.axiom(\"BornIn(RobertDeNiro2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(RobertDeNiro2000, SaudiArabia2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(RobertDeNiro2000, France2000)\")\n",
        "ltnw.axiom(\"BornIn(AlPacino2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(AlPacino2000, Netherlands2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(AlPacino2000, Poland2000)\")\n",
        "ltnw.axiom(\"BornIn(DanielDay-Lewis2000, UnitedKingdom2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(DanielDay-Lewis2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(DanielDay-Lewis2000, SaudiArabia2000)\")\n",
        "ltnw.axiom(\"BornIn(DustinHoffman2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(DustinHoffman2000, France2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(DustinHoffman2000, Iran2000)\")\n",
        "ltnw.axiom(\"BornIn(TomHanks2000, UnitedStates2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(TomHanks2000, Italy2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(TomHanks2000, Germany2000)\")\n",
        "ltnw.axiom(\"BornIn(AnthonyHopkins2000, UnitedKingdom2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(AnthonyHopkins2000, Italy2000)\")\n",
        "ltnw.axiom(\" ~ BornIn(AnthonyHopkins2000, Germany2000)\")\n",
        "ltnw.axiom(\"BornIn(MarlonBrando2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(MarlonBrando2005, Australia2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(MarlonBrando2005, Iran2005)\")\n",
        "ltnw.axiom(\"BornIn(JackNicholson2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(JackNicholson2005, France2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(JackNicholson2005, Iraq2005)\")\n",
        "ltnw.axiom(\"BornIn(RobertDeNiro2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(RobertDeNiro2005, SaudiArabia2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(RobertDeNiro2005, France2005)\")\n",
        "ltnw.axiom(\"BornIn(AlPacino2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(AlPacino2005, Netherlands2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(AlPacino2005, Poland2005)\")\n",
        "ltnw.axiom(\"BornIn(DanielDay-Lewis2005, UnitedKingdom2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(DanielDay-Lewis2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(DanielDay-Lewis2005, SaudiArabia2005)\")\n",
        "ltnw.axiom(\"BornIn(DustinHoffman2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(DustinHoffman2005, France2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(DustinHoffman2005, Iran2005)\")\n",
        "ltnw.axiom(\"BornIn(TomHanks2005, UnitedStates2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(TomHanks2005, Italy2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(TomHanks2005, Germany2005)\")\n",
        "ltnw.axiom(\"BornIn(AnthonyHopkins2005, UnitedKingdom2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(AnthonyHopkins2005, Italy2005)\")\n",
        "ltnw.axiom(\" ~ BornIn(AnthonyHopkins2005, Germany2005)\")\n",
        "\n",
        "ltnw.axiom(\"forall ?a : State(?a) -> ~ Actor(?a) & ~ Company(?a)\")\n",
        "ltnw.axiom(\"forall ?a : Actor(?a) -> ~ State(?a) & ~ Company(?a)\")\n",
        "ltnw.axiom(\"forall ?a : Company(?a) -> ~ State(?a) & ~ Actor(?a)\")\n",
        "ltnw.axiom(\"forall ?a : Actor(?a) -> Human(?a)\")\n",
        "ltnw.axiom(\"forall ?a, ?b : BornIn(?a, ?b) -> Actor(?a) & State(?b)\")\n",
        "\n",
        "ltnw.axiom(\"forall ?a, ?b : BornIn(?a, ?b) -> ~ BornIn(?b, ?a)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ7RwmFD22w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger()\n",
        "logger.basicConfig = logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "ltnw.initialize_knowledgebase(optimizer=tf.train.AdamOptimizer())\n",
        "ltnw.train(max_epochs = 5000, track_sat_levels = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZXqSvda4SFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unseen_states = [\"Afghanistan\", \"Belgium\", \"Brazil\", \"Egypt\", \"Portugal\"]\n",
        "unseen_actors = [\"DenzelWashington\", \"SeanPenn\", \"MorganFreeman\", \"JeffBridges\"]\n",
        "unseen_companies = [\"Samsung\", \"Facebook\", \"Oracle\", \"Sony\", \"Motorola\"]\n",
        "\n",
        "\n",
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if seen state in seen time slice is still a state\")\n",
        "  out_file.write(\"Check if seen state in seen time slice is still a state\\n\")\n",
        "  for state in states:\n",
        "    if \"{}2000\".format(state) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"State({}2000)\".format(state))[0], 2))\n",
        "      print(\"{} {}\".format(state, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(state, pred))\n",
        "\n",
        "  print(\"Check if seen state in unseen time slice is still a state\")\n",
        "  out_file.write(\"Check if seen state in unseen time slice is still a state\\n\")\n",
        "  for state in states:\n",
        "    if \"{}2010\".format(state) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"State({}2010)\".format(state))[0], 2))\n",
        "      print(\"{} {}\".format(state, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(state, pred))\n",
        "\n",
        "  print(\"Check if unseen state in seen time slice is still a state\")\n",
        "  out_file.write(\"Check if unseen state in seen time slice is still a state\\n\")\n",
        "  for state in unseen_states:\n",
        "    if \"{}2000\".format(state) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"State({}2000)\".format(state))[0], 2))\n",
        "      print(\"{} {}\".format(state, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(state, pred))\n",
        "\n",
        "  print(\"Check if unseen state in unseen time slice is still a state\")\n",
        "  out_file.write(\"Check if unseen state in unseen time slice is still a state\\n\")\n",
        "  for state in unseen_states:\n",
        "    if \"{}2010\".format(state) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"State({}2010)\".format(state))[0], 2))\n",
        "      print(\"{} {}\".format(state, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(state, pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Check if seen actor in seen time slice is still a actor\")\n",
        "  out_file.write(\"Check if seen actor in seen time slice is still a actor\\n\")\n",
        "  for actor in actors:\n",
        "    if \"{}2000\".format(actor) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Actor({}2000)\".format(actor))[0], 2))\n",
        "      print(\"{} {}\".format(actor, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(actor, pred))\n",
        "\n",
        "  print(\"Check if seen actor in unseen time slice is still a actor\")\n",
        "  out_file.write(\"Check if seen actor in unseen time slice is still a actor\\n\")\n",
        "  for actor in actors:\n",
        "    if \"{}2010\".format(actor) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Actor({}2010)\".format(actor))[0], 2))\n",
        "      print(\"{} {}\".format(actor, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(actor, pred))\n",
        "\n",
        "  print(\"Check if unseen actor in seen time slice is still a actor\")\n",
        "  out_file.write(\"Check if unseen actor in seen time slice is still a actor\\n\")\n",
        "  for actor in unseen_actors:\n",
        "    if \"{}2000\".format(actor) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Actor({}2000)\".format(actor))[0], 2))\n",
        "      print(\"{} {}\".format(actor, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(actor, pred))\n",
        "\n",
        "  print(\"Check if unseen actor in unseen time slice is still a actor\")\n",
        "  out_file.write(\"Check if unseen actor in unseen time slice is still a actor\\n\")\n",
        "  for actor in unseen_actors:\n",
        "    if \"{}2010\".format(actor) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Actor({}2010)\".format(actor))[0], 2))\n",
        "      print(\"{} {}\".format(actor, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(actor, pred))\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Check if seen company in seen time slice is still a company\")\n",
        "  out_file.write(\"Check if seen company in seen time slice is still a company\\n\")\n",
        "  for company in companies:\n",
        "    if \"{}2000\".format(company) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Company({}2000)\".format(company))[0], 2))\n",
        "      print(\"{} {}\".format(company, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(company, pred))\n",
        "\n",
        "  print(\"Check if seen company in unseen time slice is still a company\")\n",
        "  out_file.write(\"Check if seen company in unseen time slice is still a company\\n\")\n",
        "  for company in companies:\n",
        "    if \"{}2010\".format(company) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Company({}2010)\".format(company))[0], 2))\n",
        "      print(\"{} {}\".format(company, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(company, pred))\n",
        "\n",
        "  print(\"Check if unseen company in seen time slice is still a company\")\n",
        "  out_file.write(\"Check if unseen company in seen time slice is still a company\\n\")\n",
        "  for company in unseen_companies:\n",
        "    if \"{}2000\".format(company) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Company({}2000)\".format(company))[0], 2))\n",
        "      print(\"{} {}\".format(company, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(company, pred))\n",
        "\n",
        "  print(\"Check if unseen company in unseen time slice is still a company\")\n",
        "  out_file.write(\"Check if unseen company in unseen time slice is still a company\\n\")\n",
        "  for company in unseen_companies:\n",
        "    if \"{}2010\".format(company) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Company({}2010)\".format(company))[0], 2))\n",
        "      print(\"{} {}\".format(company, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(company, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8SsvI17dhM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if types are separated correclty in seen time slice\")\n",
        "  out_file.write(\"Check if types are separated correclty in seen time slice\\n\")\n",
        "  print(\"not Companies\")\n",
        "  out_file.write(\"not Companies\\n\")\n",
        "  for element in states + actors + unseen_actors + unseen_states:\n",
        "    if \"{}2000\".format(element) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Company({}2000)\".format(element))[0], 2))\n",
        "      print(\"{} {}\".format(element, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(element, pred))\n",
        "  print(\"not States\")\n",
        "  out_file.write(\"not States\\n\")\n",
        "  for element in companies + actors + unseen_companies + unseen_actors:\n",
        "    if \"{}2000\".format(element) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"State({}2000)\".format(element))[0], 2))\n",
        "      print(\"{} {}\".format(element, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(element, pred))\n",
        "  print(\"not Actors\")\n",
        "  out_file.write(\"not Actors\\n\")\n",
        "  for element in states + companies + unseen_companies + unseen_states:\n",
        "    if \"{}2000\".format(element) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Actor({}2000)\".format(element))[0], 2))\n",
        "      print(\"{} {}\".format(element, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(element, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcyYNcOwdiiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if types are separated correclty in unseen time slice\")\n",
        "  out_file.write(\"Check if types are separated correclty in unseen time slice\\n\")\n",
        "  print(\"not Companies\")\n",
        "  out_file.write(\"not Companies\\n\")\n",
        "  for element in states + actors + unseen_actors + unseen_states:\n",
        "    if \"{}2010\".format(element) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Company({}2010)\".format(element))[0], 2))\n",
        "      print(\"{} {}\".format(element, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(element, pred))\n",
        "  print(\"not States\")\n",
        "  out_file.write(\"not States\\n\")\n",
        "  for element in companies + actors + unseen_companies + unseen_actors:\n",
        "    if \"{}2010\".format(element) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"State({}2010)\".format(element))[0], 2))\n",
        "      print(\"{} {}\".format(element, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(element, pred))\n",
        "  print(\"not Actors\")\n",
        "  out_file.write(\"not Actors\\n\")\n",
        "  for element in states + companies + unseen_companies + unseen_states:\n",
        "    if \"{}2010\".format(element) in ltn_entities:\n",
        "      pred = str(round(ltnw.ask(\"Actor({}2010)\".format(element))[0], 2))\n",
        "      print(\"{} {}\".format(element, pred))\n",
        "      out_file.write(\"{} {} \\n\".format(element, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyrx19uddk_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if multy-entity relations are learned correctly\")\n",
        "  out_file.write(\"Check if multy-entity relations are learned correctly\\n\")\n",
        "  pred = str(round(ltnw.ask(\"BornIn(MarlonBrando1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(MarlonBrando1995, Australia1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(MarlonBrando1995, Iran1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(JackNicholson1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(JackNicholson1995, France1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(JackNicholson1995, Iraq1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(RobertDeNiro1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(RobertDeNiro1995, SaudiArabia1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(RobertDeNiro1995, France1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(AlPacino1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AlPacino1995, Netherlands1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AlPacino1995, Poland1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(DanielDay-Lewis1995, UnitedKingdom1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DanielDay-Lewis1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DanielDay-Lewis1995, SaudiArabia1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(DustinHoffman1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DustinHoffman1995, France1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DustinHoffman1995, Iran1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(TomHanks1995, UnitedStates1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(TomHanks1995, Italy1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(TomHanks1995, Germany1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(AnthonyHopkins1995, UnitedKingdom1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AnthonyHopkins1995, Italy1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AnthonyHopkins1995, Germany1995)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfqy8rjgdlnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if multy entity relations survives time\")\n",
        "  out_file.write(\"Check if multy entity relations survives time\\n\")\n",
        "  pred = str(round(ltnw.ask(\"BornIn(MarlonBrando2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(MarlonBrando2010, Australia2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(MarlonBrando2010, Iran2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(JackNicholson2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(JackNicholson2010, France2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(JackNicholson2010, Iraq2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(RobertDeNiro2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(RobertDeNiro2010, SaudiArabia2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(RobertDeNiro2010, France2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(AlPacino2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AlPacino2010, Netherlands2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AlPacino2010, Poland2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(DanielDay-Lewis2010, UnitedKingdom2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DanielDay-Lewis2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DanielDay-Lewis2010, SaudiArabia2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(DustinHoffman2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DustinHoffman2010, France2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(DustinHoffman2010, Iran2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(TomHanks2010, UnitedStates2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(TomHanks2010, Italy2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(TomHanks2010, Germany2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"BornIn(AnthonyHopkins2010, UnitedKingdom2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AnthonyHopkins2010, Italy2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\" ~ BornIn(AnthonyHopkins2010, Germany2010)\")[0], 2))\n",
        "  print(\"{}\".format(pred))\n",
        "  out_file.write(\"{}\\n\".format(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zl397MWdm76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if multy entity relations works on unseen entities\")\n",
        "  out_file.write(\"Check if multy entity relations works on unseen entities\\n\")\n",
        "  for actor in unseen_actors:\n",
        "    pred = str(round(ltnw.ask(\"BornIn({}1995, UnitedStates1995)\".format(actor))[0], 2))\n",
        "    print(\"{} {}\".format(actor, pred))\n",
        "    out_file.write(\"{} {}\\n\".format(actor, pred))\n",
        "\n",
        "  print(\"Check if multy entity relations survives time on unseen entities\")\n",
        "  out_file.write(\"Check if multy entity relations survives time on unseen entities\\n\")\n",
        "  for actor in unseen_actors:\n",
        "    pred = str(round(ltnw.ask(\"BornIn({}2010, UnitedStates2010)\".format(actor))[0], 2))\n",
        "    print(\"{} {}\".format(actor, pred))\n",
        "    out_file.write(\"{} {}\\n\".format(actor, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQgy0LrNdoT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if rules are learned\")\n",
        "  out_file.write(\"Check if rules are learned\\n\")\n",
        "  rule = \"forall ?a : State(?a) -> ~ Actor(?a) & ~ Company(?a)\"\n",
        "  pred = str(round(ltnw.ask(rule)[0], 2))\n",
        "  print(\"{} {}\".format(rule, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(rule, pred))\n",
        "  rule = \"forall ?a : Actor(?a) -> ~ State(?a) & ~ Company(?a)\"\n",
        "  pred = str(round(ltnw.ask(rule)[0], 2))\n",
        "  print(\"{} {}\".format(rule, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(rule, pred))\n",
        "  rule = \"forall ?a : Company(?a) -> ~ State(?a) & ~ Company(?a)\"\n",
        "  pred = str(round(ltnw.ask(rule)[0], 2))\n",
        "  print(\"{} {}\".format(rule, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(rule, pred))\n",
        "  rule = \"forall ?a : Actor(?a) -> Human(?a)\"\n",
        "  pred = str(round(ltnw.ask(rule)[0], 2))\n",
        "  print(\"{} {}\".format(rule, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(rule, pred))\n",
        "  rule = \"forall ?a, ?b : BornIn(?a, ?b) -> Actor(?a) & State(?b)\"\n",
        "  pred = str(round(ltnw.ask(rule)[0], 2))\n",
        "  print(\"{} {}\".format(rule, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(rule, pred))\n",
        "  rule = \"forall ?a, ?b : BornIn(?a, ?b) -> ~ BornIn(?b, ?a)\"\n",
        "  pred = str(round(ltnw.ask(rule)[0], 2))\n",
        "  print(\"{} {}\".format(rule, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(rule, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLYEn3tU5lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN.txt\", \"a\") as out_file:\n",
        "  print(\"Check if seen unseen human is classified as actor\")\n",
        "  out_file.write(\"Check if seen unseen human is classified as actor\\n\")\n",
        "  pred = str(round(ltnw.ask(\"Actor(BillClinton2000)\")[0], 2))\n",
        "  print(\"BillClinton {}\".format(pred))\n",
        "  out_file.write(\"BillClinton {} \\n\".format(pred))\n",
        "  pred = str(round(ltnw.ask(\"Actor(BarackObama2000)\")[0], 2))\n",
        "  print(\"BarackObama {}\".format(pred))\n",
        "  out_file.write(\"BarackObama {} \\n\".format(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-hWSdEDuEyo",
        "colab_type": "text"
      },
      "source": [
        "## Second batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_PaKzazuIPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltnw._reset()\n",
        "ltn_entities = set()\n",
        "\n",
        "# DEFINE COSTANTS\n",
        "# Need to load ALL entites that we will use, not only the ones uset for training the LTN\n",
        "for entity in rel_ent_1995:\n",
        "    ltnw.constant(\"{}1995\".format(clean_entity_name(entity)), model1995[entity])\n",
        "    ltn_entities.add(\"{}1995\".format(clean_entity_name(entity)))\n",
        "for entity in rel_ent_2000:\n",
        "    ltnw.constant(\"{}2000\".format(clean_entity_name(entity)), model2000[entity])\n",
        "    ltn_entities.add(\"{}2000\".format(clean_entity_name(entity)))\n",
        "for entity in rel_ent_2005:\n",
        "    ltnw.constant(\"{}2005\".format(clean_entity_name(entity)), model2005[entity])\n",
        "    ltn_entities.add(\"{}2005\".format(clean_entity_name(entity)))\n",
        "for entity in rel_ent_2010:\n",
        "    ltnw.constant(\"{}2010\".format(clean_entity_name(entity)), model2010[entity])\n",
        "    ltn_entities.add(\"{}2010\".format(clean_entity_name(entity)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZOYW3FruIZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE PREDICATE\n",
        "ltnw.predicate(\"State\", 1 * embedding_size)\n",
        "ltnw.predicate(\"Human\", 1 * embedding_size)\n",
        "ltnw.predicate(\"NatoPresence\", 1 * embedding_size)\n",
        "ltnw.predicate(\"CivilWar\", 1 * embedding_size)\n",
        "ltnw.predicate(\"LeaderOf\", 2 * embedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xolcc0euIcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE VARIABLES\n",
        "# 250 is a good compromise between time and performance\n",
        "ltnw.variable(\"?a\", np.random.uniform(min_dom, max_dom, (250, embedding_size)).astype(\"float32\"))\n",
        "ltnw.variable(\"?b\", np.random.uniform(min_dom, max_dom, (250, embedding_size)).astype(\"float32\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4AnI4nYuIey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = [\"BosniaandHerzegovina\", \"Albania\", \"Kosovo\", \"Afghanistan\", \n",
        "          \"Croatia\", \"Serbia\", \"UnitedStates\", \"Netherlands\", \"Poland\", \n",
        "          \"Australia\", \"Iran\", \"Iraq\", \"Italy\", \"Germany\", \n",
        "          \"France\", \"Slovenia\", \"Burundi\"]\n",
        "humans = [\"Hamid_Karzai\", \"NicolasSarkozy\", \"JacquesChirac\", \"BarackObama\",\n",
        "          \"HelmutKohl\", \"AngelaMerkel\", \"BillClinton\", \"GeorgeWBush\"]\n",
        "\n",
        "for state in states:\n",
        "  for year in [\"1995\", \"2000\", \"2005\", \"2010\"]:\n",
        "    if state + year in ltn_entities:\n",
        "      ltnw.axiom(\"State({})\".format(state + year))\n",
        "      ltnw.axiom(\"~ Human({})\".format(state + year))\n",
        "\n",
        "for human in humans:\n",
        "  for year in [\"1995\", \"2000\", \"2005\", \"2010\"]:\n",
        "    if human + year in ltn_entities:\n",
        "      ltnw.axiom(\"Human({})\".format(human + year))\n",
        "      ltnw.axiom(\"~ State({})\".format(human + year))\n",
        "\n",
        "ltnw.axiom(\"NatoPresence(Afghanistan2005)\")\n",
        "ltnw.axiom(\"NatoPresence(Afghanistan2010)\")\n",
        "ltnw.axiom(\"NatoPresence(Iraq2005)\")\n",
        "ltnw.axiom(\"NatoPresence(Iraq2010)\")\n",
        "ltnw.axiom(\"NatoPresence(BosniaandHerzegovina1995)\")\n",
        "ltnw.axiom(\"NatoPresence(BosniaandHerzegovina2000)\")\n",
        "ltnw.axiom(\"NatoPresence(Kosovo2000)\")\n",
        "\n",
        "ltnw.axiom(\"~ NatoPresence(Afghanistan2000)\")\n",
        "ltnw.axiom(\"~ NatoPresence(Iraq1995)\")\n",
        "ltnw.axiom(\"~ NatoPresence(BosniaandHerzegovina2005)\")\n",
        "ltnw.axiom(\"~ NatoPresence(Kosovo1995)\")\n",
        "ltnw.axiom(\"~ NatoPresence(France2000)\")\n",
        "ltnw.axiom(\"~ NatoPresence(Italy2005)\")\n",
        "ltnw.axiom(\"~ NatoPresence(UnitedStates1995)\")\n",
        "ltnw.axiom(\"~ NatoPresence(Germany2010)\")\n",
        "\n",
        "\n",
        "ltnw.axiom(\"CivilWar(Slovenia2000)\")\n",
        "ltnw.axiom(\"CivilWar(Croatia2000)\")\n",
        "ltnw.axiom(\"CivilWar(BosniaandHerzegovina2000)\")\n",
        "ltnw.axiom(\"CivilWar(Slovenia1995)\")\n",
        "ltnw.axiom(\"CivilWar(Croatia1995)\")\n",
        "ltnw.axiom(\"CivilWar(BosniaandHerzegovina1995)\")\n",
        "ltnw.axiom(\"CivilWar(Serbia1995)\")\n",
        "ltnw.axiom(\"CivilWar(Afghanistan2000)\")\n",
        "ltnw.axiom(\"CivilWar(Afghanistan1995)\")\n",
        "ltnw.axiom(\"CivilWar(Burundi1995)\")\n",
        "ltnw.axiom(\"CivilWar(Burundi2000)\")\n",
        "ltnw.axiom(\"CivilWar(Burundi2005)\")\n",
        "ltnw.axiom(\"CivilWar(Iraq1995)\")\n",
        "\n",
        "ltnw.axiom(\"~ CivilWar(France2000)\")\n",
        "ltnw.axiom(\"~ CivilWar(Italy2005)\")\n",
        "ltnw.axiom(\"~ CivilWar(UnitedStates1995)\")\n",
        "ltnw.axiom(\"~ CivilWar(Germany2010)\")\n",
        "ltnw.axiom(\"~ CivilWar(Australia2005)\")\n",
        "ltnw.axiom(\"~ CivilWar(UnitedKingdom1995)\")\n",
        "ltnw.axiom(\"~ CivilWar(UnitedStates2010)\")\n",
        "ltnw.axiom(\"~ CivilWar(Netherlands2010)\")\n",
        "ltnw.axiom(\"~ CivilWar(Iraq2010)\")\n",
        "ltnw.axiom(\"~ CivilWar(Afghanistan2010)\")\n",
        "ltnw.axiom(\"~ CivilWar(Serbia2005)\")\n",
        "ltnw.axiom(\"~ CivilWar(Slovenia2010)\")\n",
        "\n",
        "\n",
        "ltnw.axiom(\"LeaderOf(HamidKarzai2005, Afghanistan2005)\")\n",
        "ltnw.axiom(\"LeaderOf(HamidKarzai2010, Afghanistan2010)\")\n",
        "ltnw.axiom(\"LeaderOf(NicolasSarkozy2010, France2010)\")\n",
        "ltnw.axiom(\"LeaderOf(JacquesChirac1995, France1995)\")\n",
        "ltnw.axiom(\"LeaderOf(JacquesChirac2000, France2000)\")\n",
        "ltnw.axiom(\"LeaderOf(JacquesChirac2005, France2005)\")\n",
        "ltnw.axiom(\"LeaderOf(AngelaMerkel2005 , Germany2005)\")\n",
        "ltnw.axiom(\"LeaderOf(AngelaMerkel2010 , Germany2010)\")\n",
        "ltnw.axiom(\"LeaderOf(HelmutKohl1995 , Germany1995)\")\n",
        "ltnw.axiom(\"LeaderOf(BillClinton1995 , UnitedStates1995)\")\n",
        "ltnw.axiom(\"LeaderOf(BillClinton2000 , UnitedStates2000)\")\n",
        "ltnw.axiom(\"LeaderOf(GeorgeWBush2005 , UnitedStates2005)\")\n",
        "ltnw.axiom(\"LeaderOf(BarackObama2010 , UnitedStates2010)\")\n",
        "\n",
        "\n",
        "ltnw.axiom(\"~LeaderOf(JacquesChirac2000, Afghanistan2005)\")\n",
        "ltnw.axiom(\"~LeaderOf(BillClinton2000, Afghanistan2010)\")\n",
        "ltnw.axiom(\"~LeaderOf(AngelaMerkel2010, France2010)\")\n",
        "ltnw.axiom(\"~LeaderOf(BillClinton2000, France1995)\")\n",
        "ltnw.axiom(\"~LeaderOf(GeorgeWBush2005, France2000)\")\n",
        "ltnw.axiom(\"~LeaderOf(HelmutKohl1995, France2005)\")\n",
        "ltnw.axiom(\"~LeaderOf(JacquesChirac2000 , Germany2005)\")\n",
        "ltnw.axiom(\"~LeaderOf(AngelaMerkel2010 , Germany2010)\")\n",
        "ltnw.axiom(\"~LeaderOf(GeorgeWBush2005, Germany1995)\")\n",
        "ltnw.axiom(\"~LeaderOf(HamidKarzai2005 , UnitedStates1995)\")\n",
        "ltnw.axiom(\"~LeaderOf(JacquesChirac2000 , UnitedStates2000)\")\n",
        "ltnw.axiom(\"~LeaderOf(BillClinton2000 , UnitedStates2005)\")\n",
        "ltnw.axiom(\"~LeaderOf(AngelaMerkel2010 , UnitedStates2010)\")\n",
        "\n",
        "\n",
        "ltnw.axiom(\"forall ?a : State(?a) -> ~ Human(?a)\")\n",
        "ltnw.axiom(\"forall ?a : Human(?a) -> ~ State(?a) & ~ NatoPresence(?a)\")\n",
        "ltnw.axiom(\"forall ?a : NatoPresence(?a) -> State(?a)\")\n",
        "ltnw.axiom(\"forall ?a : CivilWar(?a) -> State(?a)\")\n",
        "ltnw.axiom(\"forall ?a, ?b : LeaderOf(?a, ?b) -> Human(?a) & State(?b)\")\n",
        "ltnw.axiom(\"forall ?a, ?b : LeaderOf(?a, ?b) -> ~ LeaderOf(?b, ?a)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYKu6AcQuIhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger()\n",
        "logger.basicConfig = logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "ltnw.initialize_knowledgebase(optimizer=tf.train.AdamOptimizer())\n",
        "ltnw.train(max_epochs = 5000, track_sat_levels = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_13c0deBNPGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN2.txt\", \"a\") as out_file:\n",
        "  print(\"Check if the network has learned axioms\")\n",
        "  out_file.write(\"Check if the network has learned axioms\\n\")\n",
        "  print(\"NatoPresence\")\n",
        "  out_file.write(\"NatoPresence\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(Afghanistan2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(Afghanistan2010)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(Iraq2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(Iraq2010)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(BosniaandHerzegovina1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(BosniaandHerzegovina2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence(Kosovo2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  print(\"\\n\")\n",
        "  print(\"CivilWar\")\n",
        "  out_file.write(\"CivilWar\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Slovenia2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Croatia2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(BosniaandHerzegovina2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Slovenia1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Croatia1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(BosniaandHerzegovina1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Serbia1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Afghanistan2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Afghanistan1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Burundi1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Burundi2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Burundi2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"CivilWar(Iraq1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  print(\"\\n\")\n",
        "  print(\"LeaderOf\")\n",
        "  out_file.write(\"LeaderOf\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(HamidKarzai2005, Afghanistan2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(HamidKarzai2010, Afghanistan2010)\")[0],2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(NicolasSarkozy2010, France2010)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(JacquesChirac1995, France1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(JacquesChirac2000, France2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(JacquesChirac2005, France2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(AngelaMerkel2005 , Germany2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(AngelaMerkel2010 , Germany2010)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(HelmutKohl1995 , Germany1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(BillClinton1995 , UnitedStates1995)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(BillClinton2000 , UnitedStates2000)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(GeorgeWBush2005 , UnitedStates2005)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")\n",
        "  pred = str(round(ltnw.ask(\"LeaderOf(BarackObama2010 , UnitedStates2010)\")[0], 2))\n",
        "  print(pred)\n",
        "  out_file.write(pred + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqLdk9pWNQyO",
        "colab_type": "text"
      },
      "source": [
        "Does not learn the relation LeaderOf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCpMn0fMNQcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN2.txt\", \"a\") as out_file:\n",
        "  print(\"Check if enities with semantic shifts invalidates NatoPresence predicates\")\n",
        "  out_file.write(\"Check if enities with semantic shifts invalidates NatoPresence predicates\\n\")\n",
        "  ent = \"Afghanistan1995\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Iraq1995\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"BosniaandHerzegovina2010\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Kosovo2005\" # i was wrong, NATO is still in Kosovo, the net corrected me\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  print(\"Check if NatoPresence is not overfitting on all states\")\n",
        "  out_file.write(\"Check if NatoPresence is not overfitting on all states\\n\")\n",
        "  ent = \"France2005\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Italy2000\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"UnitedStates1995\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Germany2000\"\n",
        "  pred = str(round(ltnw.ask(\"NatoPresence({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9THAylUNkqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN2.txt\", \"a\") as out_file:\n",
        "  print(\"Check if enities with semantic shifts invalidates CivilWar predicates\")\n",
        "  out_file.write(\"Check if enities with semantic shifts invalidates CivilWar predicates\\n\")\n",
        "  ent = \"Slovenia2010\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Croatia2010\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"BosniaandHerzegovina2005\" # state does not exist anymore, still make sense that it is represented near cilivwar articles\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Serbia2005\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Afghanistan2010\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Burundi2010\" # always seen with civil war, not able to generalize\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Iraq2005\"  # the net is wrong, no idea why\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbXQfQ8qXwsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN2.txt\", \"a\") as out_file:\n",
        "  print(\"Check if unsees entity usually associated with CivilWar satisfies the predicate\")\n",
        "  out_file.write(\"Check if unsees entity usually associated with CivilWar satisfies the predicate\\n\")\n",
        "  ent = \"Rwanda1995\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Rwanda2000\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Rwanda2005\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Rwanda2010\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyTPsZUHpoRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"LTN2.txt\", \"a\") as out_file:\n",
        "  print(\"Check if unsees entity without CivilWar war satisfies the predicate\")\n",
        "  out_file.write(\"Check if unsees entity without CivilWar war satisfies the predicate\\n\")\n",
        "  ent = \"Germany1995\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Germany2000\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Germany2005\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))\n",
        "  ent = \"Germany2010\"\n",
        "  pred = str(round(ltnw.ask(\"CivilWar({})\".format(ent))[0], 2))\n",
        "  print(\"{} {}\".format(ent, pred))\n",
        "  out_file.write(\"{} {}\\n\".format(ent, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw1c3PF6IEHg",
        "colab_type": "text"
      },
      "source": [
        "# **LTN RESULTS ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWdWbNDrICnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STATES analysis\n",
        "\n",
        "print(\"frequenza Nazioni\") \n",
        "entities = [\"DBRSTARTdbr:UnitedKingdomDBREND\", \"DBRSTARTdbr:NetherlandsDBREND\",\n",
        "\"DBRSTARTdbr:PolandDBREND\", \"DBRSTARTdbr:AustraliaDBREND\", \"DBRSTARTdbr:IranDBREND\", \"DBRSTARTdbr:IraqDBREND\", \"DBRSTARTdbr:UnitedStatesDBREND\", \"DBRSTARTdbr:ItalyDBREND\", \"DBRSTARTdbr:GermanyDBREND\", \"DBRSTARTdbr:FranceDBREND\"]\n",
        "printOccurancies(entities)\n",
        "\n",
        "states = ['DBRSTARTdbr:PolandDBREND', 'DBRSTARTdbr:AustraliaDBREND', 'DBRSTARTdbr:UnitedStatesDBREND']\n",
        "plotPCAovertime(states, \"States\", colors, subdirectory = 'LTN_Analysis')\n",
        "\n",
        "\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:PolandDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')     #sono sempre vicini a stati\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:AustraliaDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:UnitedStatesDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "\n",
        "print(\"frequenza Nazioni unseen\") \n",
        "entities = ['DBRSTARTdbr:AfghanistanDBREND', 'DBRSTARTdbr:BelgiumDBREND', 'DBRSTARTdbr:BrazilDBREND','DBRSTARTdbr:EgyptDBREND', 'DBRSTARTdbr:PortugalDBREND']\n",
        "printOccurancies(entities)\n",
        "\n",
        "plotPCAovertime(entities, \"States\", colors, subdirectory = 'LTN_Analysis')\n",
        "\n",
        "pcas = {1995: pca_1995, 2000: pca_2000, 2005: pca_2005, 2010: pca_2010}\n",
        "models = {1995: model1995, 2000: model2000, 2005: model2005, 2010: model2010}\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:BelgiumDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')    \n",
        "plot_evolution_NN_entity('DBRSTARTdbr:PortugalDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:AfghanistanDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:EgyptDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "\n",
        "\n",
        "# ACTOR analysis\n",
        "\n",
        "print(\"frequenza ATTORI\") \n",
        "actors = ['DBRSTARTdbr:MarlonBrandoDBREND',\n",
        "'DBRSTARTdbr:JackNicholsonDBREND', \n",
        "'DBRSTARTdbr:RobertDeNiroDBREND',\n",
        "'DBRSTARTdbr:AlPacinoDBREND',\n",
        "'DBRSTARTdbr:DanielDay-LewisDBREND',\n",
        "'DBRSTARTdbr:DustinHoffmanDBREND',\n",
        "'DBRSTARTdbr:TomHanksDBREND',\n",
        "'DBRSTARTdbr:AnthonyHopkinsDBREND']\n",
        "\n",
        "printOccurancies(actors)\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:DustinHoffmanDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:DanielDay-LewisDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:AnthonyHopkinsDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "\n",
        "unseenActor = ['DBRSTARTdbr:DenzelWashingtonDBREND',\n",
        "'DBRSTARTdbr:SeanPennDBREND',\n",
        "'DBRSTARTdbr:MorganFreemanDBREND', \n",
        "'DBRSTARTdbr:JeffBridgesDBREND'] \n",
        "\n",
        "print(\"frequenza UNSEEN ATTORI\") \n",
        "printOccurancies(unseenActor)\n",
        "\n",
        "\n",
        "\n",
        "print(\"frequenza COMPANY\") \n",
        "company = [\n",
        "'DBRSTARTdbr:AppleIncDBREND' ,\n",
        "'DBRSTARTdbr:IBMDBREND' ,\n",
        "'DBRSTARTdbr:Hewlett-PackardDBREND' ,\n",
        "'DBRSTARTdbr:DellDBREND' ,\n",
        "'DBRSTARTdbr:MicrosoftDBREND' , \n",
        "'DBRSTARTdbr:AmazoncomDBREND' , \n",
        "'DBRSTARTdbr:IntelDBREND' ,\n",
        "'DBRSTARTdbr:AdvancedMicroDevicesDBREND']\n",
        "\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:OracleDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:DellDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')\n",
        "\n",
        "printOccurancies(company)\n",
        "\n",
        "print(\"frequenza UNSEEEN COMPANY\") \n",
        "unseenCompany = [\n",
        "'DBRSTARTdbr:SamsungDBREND' ,\n",
        "'DBRSTARTdbr:OracleDBREND' ,\n",
        "'DBRSTARTdbr:SonyDBREND' ,\n",
        "'DBRSTARTdbr:MotorolaDBREND' ,\n",
        "'DBRSTARTdbr:FacebookDBREND']\n",
        "\n",
        "printOccurancies(unseenCompany)\n",
        "plot_evolution_NN_entity('DBRSTARTdbr:MotorolaDBREND', pcas, models, colors, subdirectory = 'LTN_Analysis')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}